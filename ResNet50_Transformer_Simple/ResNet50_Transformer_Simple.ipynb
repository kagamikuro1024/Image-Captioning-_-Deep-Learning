{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRD/4HHcHNNhdNZKMdkDwi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/DoanNgocToan/clean_data_flickr8k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJbn4pRW9RnY","executionInfo":{"status":"ok","timestamp":1766216257860,"user_tz":-420,"elapsed":61293,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"b2366a52-7c21-416d-9b41-744faa43ea4b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'clean_data_flickr8k'...\n","remote: Enumerating objects: 8128, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 8128 (delta 2), reused 0 (delta 0), pack-reused 8125 (from 3)\u001b[K\n","Receiving objects: 100% (8128/8128), 1.07 GiB | 21.40 MiB/s, done.\n","Resolving deltas: 100% (15/15), done.\n","Updating files: 100% (8096/8096), done.\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ll-vk2NK9Bhf","executionInfo":{"status":"ok","timestamp":1766216278772,"user_tz":-420,"elapsed":20903,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"outputs":[],"source":["import os\n","import pickle\n","import random\n","import time\n","from collections import defaultdict\n","from collections import Counter\n","import math\n","\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","\n","from nltk.translate.bleu_score import corpus_bleu\n"]},{"cell_type":"code","source":["\n","# ============================================================================\n","# CONFIG\n","# ============================================================================\n","class Config:\n","    DATASET_PATH = './clean_data_flickr8k'\n","    CAPTION_PATH = os.path.join(DATASET_PATH, 'captions.txt')\n","    IMAGE_DIR = os.path.join(DATASET_PATH, 'Images')\n","    FEATURES_PATH = os.path.join(DATASET_PATH, 'features_resnet50.pkl')\n","    MODEL_SAVE_PATH = os.path.join(DATASET_PATH, 'best_model_resnet50_transformer.pth')\n","    TENSORBOARD_LOG_ROOT = os.path.join(DATASET_PATH, 'runs')\n","\n","    # model hyperparams\n","    NUM_HEADS = 8\n","    NUM_LAYERS = 4\n","    FF_DIM = 2048\n","    EMBED_SIZE = 512\n","    DROPOUT = 0.5\n","\n","    # training\n","    BATCH_SIZE = 32\n","    EPOCHS = 30\n","    WEIGHT_DECAY = 1e-5\n","    LEARNING_RATE = 1e-4\n","    TRAIN_SPLIT = 0.90\n","\n","    LR_SCHEDULER_FACTOR = 0.5\n","    LR_SCHEDULER_PATIENCE = 2\n","    EARLY_STOPPING_PATIENCE = 15\n","\n","    NUM_WORKERS = 0\n","    PIN_MEMORY = False\n","\n","    # features\n","    FEATURE_SHAPE = (49, 2048)   # 7*7, 2048\n","    IMAGE_SIZE = (224, 224)\n","    MAX_LEN = 37\n","\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    @classmethod\n","    def print_config(cls):\n","        print(\"=\"*70)\n","        print(\"CONFIGURATION\")\n","        print(\"=\"*70)\n","        print(f\"Device: {cls.DEVICE}\")\n","        print(f\"Batch Size: {cls.BATCH_SIZE}\")\n","        print(f\"Epochs: {cls.EPOCHS}\")\n","        print(f\"Learning Rate: {cls.LEARNING_RATE}\")\n","        print(f\"Embed Size: {cls.EMBED_SIZE}\")\n","        print(f\"Num Heads: {cls.NUM_HEADS}\")\n","        print(f\"Num Layers: {cls.NUM_LAYERS}\")\n","        print(f\"FF Dimension: {cls.FF_DIM}\")\n","        print(f\"Dropout: {cls.DROPOUT}\")\n","        print(f\"Feature shape: {cls.FEATURE_SHAPE}\")\n","        print(\"=\"*70)\n"],"metadata":{"id":"GMa6kL5w9d7A","executionInfo":{"status":"ok","timestamp":1766216278775,"user_tz":-420,"elapsed":2,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# FEATURE EXTRACTOR (ResNet50 - up to conv layer)\n","# ============================================================================\n","class FeatureExtractorResNet50:\n","    def __init__(self, device=Config.DEVICE):\n","        self.device = device\n","        self.model = self._build_model()\n","        self.transform = transforms.Compose([\n","            transforms.Resize(Config.IMAGE_SIZE),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def _build_model(self):\n","        resnet = models.resnet50(pretrained=True)\n","        # take all layers except avgpool & fc -> output: (batch, 2048, 7, 7)\n","        modules = list(resnet.children())[:-2]\n","        model = nn.Sequential(*modules)\n","        model.eval().to(self.device)\n","        for p in model.parameters():\n","            p.requires_grad = False\n","        return model\n","\n","    def extract_features(self, image_dir, save_path):\n","        print(f\"\\nExtracting features (ResNet50) from {image_dir} ...\")\n","        features = {}\n","        image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n","        with torch.no_grad():\n","            for img_name in tqdm(image_files, desc=\"Extracting features\"):\n","                img_path = os.path.join(image_dir, img_name)\n","                try:\n","                    image = Image.open(img_path).convert('RGB')\n","                    img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n","                    feat = self.model(img_tensor)  # (1, 2048, 7, 7)\n","                    feat = feat.permute(0, 2, 3, 1).cpu().numpy()  # (1,7,7,2048)\n","                    image_id = img_name.split('.')[0]\n","                    features[image_id] = feat\n","                except Exception as e:\n","                    print(f\"Error processing {img_name}: {e}\")\n","        with open(save_path, 'wb') as f:\n","            pickle.dump(features, f)\n","        print(f\"Saved features -> {save_path} (total {len(features)})\")\n","        return features\n","\n"],"metadata":{"id":"61EeNch19iL_","executionInfo":{"status":"ok","timestamp":1766216278778,"user_tz":-420,"elapsed":1,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# CAPTION PROCESSOR (same idea)\n","# ============================================================================\n","class CaptionProcessor:\n","    def __init__(self, caption_path):\n","        self.caption_path = caption_path\n","        self.mapping = {}\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.max_length = 0\n","        self.vocab_size = 0\n","\n","    def load_captions(self):\n","        print(\"\\nLoading captions...\")\n","        with open(self.caption_path, 'r', encoding='utf-8') as f:\n","            next(f)\n","            for line in f:\n","                parts = line.strip().split(',', 1)\n","                if len(parts) < 2:\n","                    continue\n","                image_name, caption = parts\n","                image_id = image_name.split('.')[0]\n","                if image_id not in self.mapping:\n","                    self.mapping[image_id] = []\n","                self.mapping[image_id].append(caption)\n","        print(f\"Loaded captions for {len(self.mapping)} images\")\n","\n","    def clean_captions(self):\n","        print(\"Cleaning captions...\")\n","        for img_id, caps in self.mapping.items():\n","            for i in range(len(caps)):\n","                cap = caps[i].lower()\n","                cap = ''.join([c for c in cap if c.isalnum() or c.isspace()])\n","                cap = ' '.join(cap.split())\n","                words = [w for w in cap.split() if len(w) > 1]\n","                caps[i] = 'startseq ' + ' '.join(words) + ' endseq'\n","\n","    def build_vocabulary(self, min_freq=1):\n","        print(\"Building vocabulary...\")\n","        freq = defaultdict(int)\n","        for caps in self.mapping.values():\n","            for cap in caps:\n","                for w in cap.split():\n","                    freq[w] += 1\n","        # optionally filter by min_freq\n","        words = [w for w, c in freq.items() if c >= min_freq]\n","        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n","        idx = 2\n","        for w in sorted(words):\n","            if w in self.word2idx:\n","                continue\n","            self.word2idx[w] = idx\n","            idx += 1\n","        self.idx2word = {i: w for w, i in self.word2idx.items()}\n","        self.vocab_size = len(self.word2idx)\n","        # max length\n","        self.max_length = max(len(c.split()) for caps in self.mapping.values() for c in caps)\n","        print(f\"Vocab size: {self.vocab_size}, Max caption length: {self.max_length}\")\n","\n","    def encode_caption(self, caption):\n","        return [self.word2idx.get(w, self.word2idx['<UNK>']) for w in caption.split()]\n","\n","    def decode_caption(self, indices):\n","        words = []\n","        for idx in indices:\n","            if idx == 0:\n","                continue\n","            w = self.idx2word.get(idx, '<UNK>')\n","            if w in ('startseq', 'endseq'):\n","                continue\n","            words.append(w)\n","        return ' '.join(words)\n","\n"],"metadata":{"id":"5ZLG2x6N9nED","executionInfo":{"status":"ok","timestamp":1766216278780,"user_tz":-420,"elapsed":1,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# DATASET (caption-level: input_seq, target_seq)\n","# ============================================================================\n","class FlickrDataset(Dataset):\n","    def __init__(self, image_ids, captions_mapping, features, caption_processor):\n","        self.image_ids = image_ids\n","        self.captions_mapping = captions_mapping\n","        self.features = features\n","        self.cp = caption_processor\n","        self.samples = []\n","        for img_id in image_ids:\n","            if img_id not in features:\n","                continue\n","            for caption in captions_mapping[img_id]:\n","                encoded = caption_processor.encode_caption(caption)\n","                # input = startseq ... token_{T-1}; target = token_1 ... endseq\n","                if len(encoded) < 2:\n","                    continue\n","                inp = encoded[:-1]\n","                tgt = encoded[1:]\n","                self.samples.append((img_id, inp, tgt))\n","        # compute max length from processor\n","        self.max_len = caption_processor.max_length - 1  # because input length is caption-1\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_id, inp, tgt = self.samples[idx]\n","        # image features: (1,7,7,2048) -> reshape to (49,2048)\n","        feat = self.features[img_id].reshape(-1, Config.FEATURE_SHAPE[1])  # (49,2048)\n","        # pad input and target to self.max_len\n","        inp_arr = np.zeros(self.max_len, dtype=np.int64)\n","        tgt_arr = np.zeros(self.max_len, dtype=np.int64)\n","        inp_arr[:len(inp)] = inp\n","        tgt_arr[:len(tgt)] = tgt\n","        return (\n","            torch.FloatTensor(feat),                 # (49,2048)\n","            torch.LongTensor(inp_arr),              # (max_len,)\n","            torch.LongTensor(tgt_arr),              # (max_len,)\n","        )\n","\n"],"metadata":{"id":"gRburRtx9tGv","executionInfo":{"status":"ok","timestamp":1766216278975,"user_tz":-420,"elapsed":194,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# DATASET (caption-level: input_seq, target_seq)\n","# ============================================================================\n","class FlickrDataset(Dataset):\n","    def __init__(self, image_ids, captions_mapping, features, caption_processor):\n","        self.image_ids = image_ids\n","        self.captions_mapping = captions_mapping\n","        self.features = features\n","        self.cp = caption_processor\n","        self.samples = []\n","        for img_id in image_ids:\n","            if img_id not in features:\n","                continue\n","            for caption in captions_mapping[img_id]:\n","                encoded = caption_processor.encode_caption(caption)\n","                # input = startseq ... token_{T-1}; target = token_1 ... endseq\n","                if len(encoded) < 2:\n","                    continue\n","                inp = encoded[:-1]\n","                tgt = encoded[1:]\n","                self.samples.append((img_id, inp, tgt))\n","        # compute max length from processor\n","        self.max_len = caption_processor.max_length - 1  # because input length is caption-1\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_id, inp, tgt = self.samples[idx]\n","        # image features: (1,7,7,2048) -> reshape to (49,2048)\n","        feat = self.features[img_id].reshape(-1, Config.FEATURE_SHAPE[1])  # (49,2048)\n","        # pad input and target to self.max_len\n","        inp_arr = np.zeros(self.max_len, dtype=np.int64)\n","        tgt_arr = np.zeros(self.max_len, dtype=np.int64)\n","        inp_arr[:len(inp)] = inp\n","        tgt_arr[:len(tgt)] = tgt\n","        return (\n","            torch.FloatTensor(feat),                 # (49,2048)\n","            torch.LongTensor(inp_arr),              # (max_len,)\n","            torch.LongTensor(tgt_arr),              # (max_len,)\n","        )\n"],"metadata":{"id":"B6oMJFWj9zCI","executionInfo":{"status":"ok","timestamp":1766216278989,"user_tz":-420,"elapsed":2,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# POSITIONAL ENCODING (từ code ChatGPT)\n","# ============================================================================\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","# ============================================================================\n","# TRANSFORMER DECODER (từ code ChatGPT, sử dụng tên biến gốc)\n","# ============================================================================\n","\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_heads, num_layers, ff_dim, dropout, max_len):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n","        self.pos = PositionalEncoding(embed_size, max_len)\n","        layer = nn.TransformerDecoderLayer(embed_size, num_heads, ff_dim, dropout, batch_first=True)\n","        self.decoder = nn.TransformerDecoder(layer, num_layers)\n","        self.fc = nn.Linear(embed_size, vocab_size)\n","\n","    def forward(self, tgt, memory, tgt_mask, tgt_key_padding_mask):\n","        x = self.embed(tgt)\n","        x = self.pos(x)\n","        out = self.decoder(x, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n","        return self.fc(out) # (B, T, V)\n","\n","# ============================================================================\n","# MODEL: Encoder already precomputed; Decoder with LSTMCell + attention per timestep\n","# ============================================================================\n","class ImageCaptioningModel(nn.Module): # Giữ nguyên tên lớp để tương thích Trainer\n","    def __init__(self, vocab_size, embed_size=Config.EMBED_SIZE,\n","                 num_heads=Config.NUM_HEADS, num_layers=Config.NUM_LAYERS,\n","                 ff_dim=Config.FF_DIM, feature_dim=Config.FEATURE_SHAPE[1],\n","                 dropout=Config.DROPOUT, max_len = Config.MAX_LEN):\n","        super(ImageCaptioningModel, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.feature_dim = feature_dim\n","        self.max_len = max_len # Lấy từ cp.max_length\n","\n","        # 1. Feature Projection (chuyển đổi 2048 -> EMBED_SIZE)\n","        # ResNet50 output is (B, 2048, 7, 7) -> (B, 49, 2048)\n","        self.proj = nn.Linear(feature_dim, embed_size)\n","\n","        # 2. Transformer Decoder\n","        self.decoder = TransformerDecoder(\n","            vocab_size, embed_size, num_heads, num_layers, ff_dim, dropout, max_len\n","        )\n","\n","    def generate_mask(self, sz, device):\n","        # MASK: upper triangle to -inf\n","        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(device)\n","\n","    def forward(self, image_features, input_seqs):\n","        \"\"\"\n","        image_features: (batch, 49, feature_dim=2048) -> output của ResNet50 đã reshape\n","        input_seqs: (batch, max_len-1) - chuỗi input (startseq ... token_{T-1})\n","        returns: outputs: (batch, max_len-1, vocab_size)\n","        \"\"\"\n","        device = image_features.device\n","\n","        # 1. Image Features Projection (Memory for Decoder)\n","        # image_features: (B, 49, 2048)\n","        memory = self.proj(image_features) # (B, 49, embed_size)\n","\n","        # 2. Prepare Target (Input Sequence)\n","        # T là max_len của input_seqs (max_len_caption - 1)\n","        T = input_seqs.size(1)\n","\n","        # 3. Create Masks\n","        # tgt_mask: MASK CHE TƯƠNG LAI\n","        tgt_mask = self.generate_mask(T, device)\n","        # tgt_pad_mask: MASK CHE PAD_TOKEN (index 0)\n","        tgt_pad_mask = (input_seqs == 0) # (B, T)\n","\n","        # 4. Decode\n","        outputs = self.decoder(\n","            input_seqs,\n","            memory, # Image Features (Encoder Output)\n","            tgt_mask,\n","            tgt_pad_mask\n","        ) # (B, T, V)\n","\n","        return outputs\n"],"metadata":{"id":"LsVuE5FA93mn","executionInfo":{"status":"ok","timestamp":1766216278993,"user_tz":-420,"elapsed":1,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# TRAINER\n","# ============================================================================\n","class Trainer:\n","    def __init__(self, model, train_loader, val_loader, caption_processor, config=Config):\n","        self.model = model.to(config.DEVICE)\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.cp = caption_processor\n","        self.config = config\n","\n","        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=config.LEARNING_RATE)\n","        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, mode='min', factor=config.LR_SCHEDULER_FACTOR,\n","            patience=config.LR_SCHEDULER_PATIENCE\n","        )\n","\n","        self.best_val_loss = float('inf')\n","        self.epochs_no_improve = 0\n","        log_dir = os.path.join(\n","            config.TENSORBOARD_LOG_ROOT,\n","            time.strftime(\"%b%d_%H-%M-%S\") + f'_{config.DEVICE.type}'\n","        )\n","        # Sử dụng log_dir mới này cho SummaryWriter\n","        self.writer = SummaryWriter(log_dir)\n","\n","    def train_epoch(self, epoch):\n","        self.model.train()\n","        total_loss = 0.0\n","        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.EPOCHS} [Train]\")\n","        for batch_idx, (img_feats, input_seqs, target_seqs) in enumerate(pbar):\n","            img_feats = img_feats.to(self.config.DEVICE)             # (B,49,2048)\n","            input_seqs = input_seqs.to(self.config.DEVICE)           # (B,max_len)\n","            target_seqs = target_seqs.to(self.config.DEVICE)         # (B,max_len)\n","\n","            outputs = self.model(img_feats, input_seqs)     # (B,max_len,vocab)\n","            # compute loss: flatten\n","            B, T, V = outputs.size()\n","            outputs_flat = outputs.view(B * T, V)\n","            targets_flat = target_seqs.view(B * T)\n","\n","            loss = self.criterion(outputs_flat, targets_flat)\n","\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n","            self.optimizer.step()\n","\n","            total_loss += loss.item()\n","            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","            global_step = epoch * len(self.train_loader) + batch_idx\n","            self.writer.add_scalar('Train/Loss_batch', loss.item(), global_step)\n","        avg_epoch_loss = total_loss / len(self.train_loader)\n","        self.writer.add_scalar('Train/Loss_epoch', avg_epoch_loss, epoch)\n","        return total_loss / len(self.train_loader)\n","\n","    def validate(self, epoch):\n","        self.model.eval()\n","        total_loss = 0.0\n","        with torch.no_grad():\n","            pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{self.config.EPOCHS} [Val]\")\n","            for img_feats, input_seqs, target_seqs in pbar:\n","                img_feats = img_feats.to(self.config.DEVICE)\n","                input_seqs = input_seqs.to(self.config.DEVICE)\n","                target_seqs = target_seqs.to(self.config.DEVICE)\n","\n","                outputs = self.model(img_feats, input_seqs)\n","                B, T, V = outputs.size()\n","                outputs_flat = outputs.view(B * T, V)\n","                targets_flat = target_seqs.view(B * T)\n","\n","                loss = self.criterion(outputs_flat, targets_flat)\n","                total_loss += loss.item()\n","                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        avg_loss = total_loss / len(self.val_loader)\n","        self.writer.add_scalar('Val/Loss_epoch', avg_loss, epoch)\n","        return avg_loss\n","\n","    def save_checkpoint(self, epoch, val_loss):\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'val_loss': val_loss,\n","            'vocab_size': self.cp.vocab_size,\n","            'max_length': self.cp.max_length,\n","            'word2idx': self.cp.word2idx,\n","            'idx2word': self.cp.idx2word\n","        }\n","        torch.save(checkpoint, self.config.MODEL_SAVE_PATH)\n","        print(f\"Model saved to {self.config.MODEL_SAVE_PATH}\")\n","\n","    def train(self):\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"STARTING TRAINING\")\n","        print(\"=\"*70)\n","        for epoch in range(self.config.EPOCHS):\n","            train_loss = self.train_epoch(epoch)\n","            val_loss = self.validate(epoch)\n","\n","            print(f\"\\nEpoch {epoch+1}/{self.config.EPOCHS}\")\n","            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","            print(f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n","\n","            self.scheduler.step(val_loss)\n","\n","            if val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.epochs_no_improve = 0\n","                self.save_checkpoint(epoch, val_loss)\n","                print(f\"✓ New best model! Val Loss: {val_loss:.4f}\")\n","            else:\n","                self.epochs_no_improve += 1\n","                print(f\"No improvement for {self.epochs_no_improve} epoch(s)\")\n","            self.writer.add_scalar('Train/LearningRate', self.optimizer.param_groups[0]['lr'], epoch)\n","\n","            if self.epochs_no_improve >= self.config.EARLY_STOPPING_PATIENCE:\n","                print(f\"Early stopping after {epoch+1} epochs\")\n","                break\n","\n","        self.writer.close()\n","        print(\"\\nTRAINING COMPLETED\\n\")\n"],"metadata":{"id":"rgCFpKPG98-X","executionInfo":{"status":"ok","timestamp":1766216278996,"user_tz":-420,"elapsed":2,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# INFERENCE: greedy and beam search\n","# ============================================================================\n","class CaptionGenerator:\n","    # Giữ nguyên __init__ và _init_hidden (vì không dùng trong Transformer)\n","    def __init__(self, model, caption_processor, features, config=Config):\n","        self.model = model.to(config.DEVICE)\n","        self.model.eval()\n","        self.cp = caption_processor\n","        self.features = features\n","        self.config = config\n","\n","        self.start_idx = self.cp.word2idx.get('startseq', 1)\n","        self.end_idx = self.cp.word2idx.get('endseq', None)\n","        if self.end_idx is None:\n","            self.end_idx = self.cp.word2idx.get('end', 0)\n","        # max_length cho inference là chiều dài max của caption - 1\n","        self.max_gen_len = self.cp.max_length - 1\n","\n","    # Hàm _init_hidden/etc. (của LSTM) KHÔNG CẦN DÙNG TRONG TRANSFORMER\n","\n","    def _prepare_features(self, image_id):\n","        feat = self.features[image_id]  # (1,7,7,2048)\n","        feat_t = torch.FloatTensor(feat).to(self.config.DEVICE)\n","        img_feats = feat_t.reshape(1, -1, Config.FEATURE_SHAPE[1])  # (1,49,2048)\n","        # Encoder output (Memory)\n","        memory = self.model.proj(img_feats) # (1, 49, embed_size)\n","        return memory # (1, 49, embed_size)\n","\n","    def generate_caption_greedy(self, image_id):\n","        memory = self._prepare_features(image_id)\n","        device = memory.device\n","\n","        # Khởi tạo chuỗi: [start_idx]\n","        generated_seq = torch.LongTensor([[self.start_idx]]).to(device) # (1, 1)\n","\n","        for t in range(self.max_gen_len):\n","            # 1. Tạo mask cho chuỗi hiện tại\n","            T = generated_seq.size(1)\n","            tgt_mask = self.model.generate_mask(T, device)\n","            tgt_pad_mask = (generated_seq == 0) # Chỉ để an toàn, không có pad ở đây\n","\n","            # 2. Forward Decoder\n","            # Logits (1, T, V)\n","            logits = self.model.decoder(\n","                generated_seq, memory, tgt_mask, tgt_pad_mask\n","            )\n","\n","            # 3. Lấy token tiếp theo (chỉ xét token cuối cùng)\n","            last_logits = logits[:, -1, :] # (1, V)\n","            next_token = last_logits.argmax(dim=-1).item() # scalar\n","\n","            if next_token == self.end_idx or next_token == 0:\n","                break\n","\n","            # 4. Thêm token vào chuỗi\n","            next_token_tensor = torch.LongTensor([[next_token]]).to(device) # (1, 1)\n","            generated_seq = torch.cat([generated_seq, next_token_tensor], dim=1) # (1, T+1)\n","\n","        # Trả về chuỗi chỉ mục (loại bỏ start_idx)\n","        final_indices = generated_seq.squeeze(0).tolist()\n","        return self.cp.decode_caption(final_indices)\n","\n","    # Logic Beam Search cho Transformer (phức tạp hơn, nhưng dựa trên ý tưởng cũ)\n","    def generate_caption_beam(self, image_id, beam_size=3):\n","        # ... (Sẽ cần triển khai Beam Search khác cho Transformer,\n","        # nhưng để giữ sự tương đồng, ta chỉ cần thay đổi bước forward\n","        # và cách lưu trạng thái. Do Transformer không có trạng thái hidden/cell\n","        # nên trạng thái là toàn bộ chuỗi đã generated)\n","\n","        memory = self._prepare_features(image_id)\n","        device = memory.device\n","\n","        # Beam: (sequence_of_indices, cumulative_logprob)\n","        # Khởi tạo: ([start_idx], 0.0)\n","        beams = [([self.start_idx], 0.0)]\n","        completed = []\n","\n","        for t in range(self.max_gen_len):\n","            new_beams = []\n","\n","            # 1. Chuẩn bị batch cho tất cả các beam (tối đa beam_size)\n","            current_sequences = [torch.LongTensor([seq]).to(device) for seq, _ in beams]\n","\n","            if not current_sequences:\n","                break\n","\n","            # Pad/Stack các chuỗi thành một batch\n","            T_max = max(s.size(1) for s in current_sequences)\n","            batched_seq = torch.zeros(len(current_sequences), T_max, dtype=torch.long, device=device)\n","            for i, seq in enumerate(current_sequences):\n","                batched_seq[i, :seq.size(1)] = seq\n","\n","            # Mở rộng memory cho batch size\n","            batched_memory = memory.expand(len(current_sequences), -1, -1)\n","\n","            # 2. Tạo mask và forward (một lần cho cả batch)\n","            T_batch = batched_seq.size(1)\n","            tgt_mask = self.model.generate_mask(T_batch, device)\n","            tgt_pad_mask = (batched_seq == 0)\n","\n","            # Logits (B_current, T_batch, V)\n","            logits = self.model.decoder(\n","                batched_seq, batched_memory, tgt_mask, tgt_pad_mask\n","            )\n","            log_probs = torch.log_softmax(logits, dim=-1) # (B_current, T_batch, V)\n","\n","            # 3. Xử lý log_probs cho token cuối cùng\n","            last_log_probs = log_probs[:, -1, :] # (B_current, V)\n","\n","            for i, (seq, score) in enumerate(beams):\n","                if seq[-1] == self.end_idx:\n","                    completed.append((seq, score))\n","                    continue\n","\n","                # Lấy top-k (beam_size) cho token tiếp theo\n","                topk_logprobs, topk_idx = torch.topk(last_log_probs[i], beam_size)\n","\n","                for k in range(beam_size):\n","                    idx_k = int(topk_idx[k].item())\n","                    lp = float(topk_logprobs[k].item())\n","                    new_seq = seq + [idx_k]\n","                    new_score = score + lp\n","                    new_beams.append((new_seq, new_score))\n","\n","            # Chọn top beam_size từ tất cả new_beams\n","            new_beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n","            beams = new_beams\n","\n","            if len(completed) >= beam_size:\n","                break\n","\n","        for seq, score in beams:\n","            completed.append((seq, score))\n","\n","        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n","        best_seq = completed[0][0]\n","        return self.cp.decode_caption(best_seq)\n","\n","    # Giữ nguyên hàm visualize_caption\n","    def visualize_caption(self, image_id, image_dir, beam_size=3):\n","        # show image + print actual & predicted\n","        img_files = [f for f in os.listdir(image_dir) if f.startswith(image_id)]\n","        if not img_files:\n","            print(f\"Image {image_id} not found in {image_dir}\")\n","            return\n","        img_path = os.path.join(image_dir, img_files[0])\n","        image = Image.open(img_path)\n","        actual = self.cp.mapping.get(image_id, [])\n","        pred_greedy = self.generate_caption_greedy(image_id)\n","        pred_beam = self.generate_caption_beam(image_id, beam_size=beam_size)\n","        print(\"\\n\" + \"=\"*60)\n","        print(f\"Image ID: {image_id}\")\n","        print(\"ACTUAL CAPTIONS:\")\n","        for i, c in enumerate(actual, 1):\n","            print(f\"{i}. {c}\")\n","        print(\"\\nPREDICTED (greedy):\")\n","        print(pred_greedy)\n","        print(\"\\nPREDICTED (beam size={}):\".format(beam_size))\n","        print(pred_beam)\n","        print(\"=\"*60 + \"\\n\")\n","        try:\n","            import matplotlib.pyplot as plt\n","            plt.figure(figsize=(8,6))\n","            plt.imshow(image)\n","            plt.axis('off')\n","            plt.show()\n","        except Exception:\n","            pass\n"],"metadata":{"id":"tzOZa_ti-AV3","executionInfo":{"status":"ok","timestamp":1766216279057,"user_tz":-420,"elapsed":60,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# EVALUATOR\n","# ============================================================================\n","class ModelEvaluator:\n","    def __init__(self, model, caption_processor, features, image_ids, config=Config):\n","        self.generator = CaptionGenerator(model, caption_processor, features, config)\n","        self.cp = caption_processor\n","        self.image_ids = image_ids\n","\n","    def evaluate(self, use_beam=False, beam_size=3):\n","        actual = []\n","        predicted = []\n","        for image_id in tqdm(self.image_ids, desc=\"Generating captions\"):\n","            if image_id not in self.cp.mapping:\n","                continue\n","            refs = [cap.split() for cap in self.cp.mapping[image_id]]\n","            if use_beam:\n","                pred = self.generator.generate_caption_beam(image_id, beam_size).split()\n","            else:\n","                pred = self.generator.generate_caption_greedy(image_id).split()\n","            actual.append(refs)\n","            predicted.append(pred)\n","        bleu1 = corpus_bleu(actual, predicted, weights=(1,0,0,0))\n","        bleu2 = corpus_bleu(actual, predicted, weights=(0.5,0.5,0,0))\n","        bleu3 = corpus_bleu(actual, predicted, weights=(0.33,0.33,0.33,0))\n","        bleu4 = corpus_bleu(actual, predicted, weights=(0.25,0.25,0.25,0.25))\n","        print(\"\\nBLEU scores:\")\n","        print(f\"BLEU-1: {bleu1:.4f}\")\n","        print(f\"BLEU-2: {bleu2:.4f}\")\n","        print(f\"BLEU-3: {bleu3:.4f}\")\n","        print(f\"BLEU-4: {bleu4:.4f}\")\n","        return {'BLEU-1':bleu1, 'BLEU-2':bleu2, 'BLEU-3':bleu3, 'BLEU-4':bleu4}\n","\n","# ============================================================================\n","# CUSTOM BLEU WITH LINEAR BP (NO NLTK)\n","# ============================================================================\n","\n","class LinearBPBLEU:\n","    \"\"\"\n","    Corpus-level BLEU with:\n","    - clipped n-gram precision (n=1..4)\n","    - geometric mean\n","    - linear BP = min(1, pred_len / ref_len)\n","    \"\"\"\n","\n","    def __init__(self, max_n=4):\n","        self.max_n = max_n\n","\n","    def _ngram_counts(self, tokens, n):\n","        return Counter([tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n","\n","    def _clipped_precision(self, references, hypotheses, n):\n","        \"\"\"\n","        references: list of list of tokens (multiple refs)\n","        hypotheses: list of tokens\n","        \"\"\"\n","        hyp_counts = self._ngram_counts(hypotheses, n)\n","        if not hyp_counts:\n","            return 0.0, 0\n","\n","        max_ref_counts = Counter()\n","        for ref in references:\n","            ref_counts = self._ngram_counts(ref, n)\n","            for ng in ref_counts:\n","                max_ref_counts[ng] = max(max_ref_counts[ng], ref_counts[ng])\n","\n","        clipped = {\n","            ng: min(count, max_ref_counts.get(ng, 0))\n","            for ng, count in hyp_counts.items()\n","        }\n","\n","        return sum(clipped.values()), sum(hyp_counts.values())\n","\n","    def corpus_bleu(self, list_of_references, hypotheses, weights=None):\n","        \"\"\"\n","        list_of_references: [[ref1_tokens, ref2_tokens, ...], ...]\n","        hypotheses: [hyp_tokens, ...]\n","        \"\"\"\n","        if weights is None:\n","            weights = [1 / self.max_n] * self.max_n\n","\n","        p_ns = []\n","        total_pred_len = 0\n","        total_ref_len = 0\n","\n","        for n in range(1, self.max_n + 1):\n","            clipped_total = 0\n","            total = 0\n","            for refs, hyp in zip(list_of_references, hypotheses):\n","                c, t = self._clipped_precision(refs, hyp, n)\n","                clipped_total += c\n","                total += t\n","\n","            if total == 0:\n","                p_ns.append(0.0)\n","            else:\n","                p_ns.append(clipped_total / total)\n","\n","        # geometric mean\n","        smooth_eps = 1e-9\n","        log_p_sum = 0.0\n","        for w, p in zip(weights, p_ns):\n","            log_p_sum += w * math.log(max(p, smooth_eps))\n","        geo_mean = math.exp(log_p_sum)\n","\n","        # length stats\n","        for refs, hyp in zip(list_of_references, hypotheses):\n","            total_pred_len += len(hyp)\n","            ref_lens = [len(r) for r in refs]\n","            total_ref_len += min(ref_lens, key=lambda rl: abs(rl - len(hyp)))\n","\n","        # LINEAR BP\n","        bp = min(1.0, total_pred_len / max(total_ref_len, 1))\n","\n","        return bp * geo_mean\n","\n","# ============================================================================\n","# RESEARCH EVALUATOR: BLEU (NLTK) + BLEU (LINEAR BP) + METEOR\n","# ============================================================================\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.meteor_score import meteor_score\n","\n","class ResearchEvaluator:\n","    def __init__(self, model, caption_processor, features, image_ids, config=Config):\n","        self.generator = CaptionGenerator(model, caption_processor, features, config)\n","        self.cp = caption_processor\n","        self.image_ids = image_ids\n","        self.linear_bleu = LinearBPBLEU(max_n=4)\n","\n","    def evaluate(self, use_beam=False, beam_size=3):\n","        actual = []\n","        predicted = []\n","        meteor_scores = []\n","        pred_lens = []\n","        ref_lens = []\n","\n","        for image_id in tqdm(self.image_ids, desc=\"Research Evaluation\"):\n","            if image_id not in self.cp.mapping:\n","                continue\n","\n","            refs = [cap.split() for cap in self.cp.mapping[image_id]]\n","\n","            if use_beam:\n","                pred_tokens = self.generator.generate_caption_beam(\n","                    image_id, beam_size\n","                ).split()\n","            else:\n","                pred_tokens = self.generator.generate_caption_greedy(image_id).split()\n","\n","            actual.append(refs)\n","            predicted.append(pred_tokens)\n","\n","            # METEOR (sentence-level)\n","            meteor_scores.append(\n","                meteor_score(refs, pred_tokens)\n","            )\n","\n","            pred_lens.append(len(pred_tokens))\n","            ref_lens.append(min(len(r) for r in refs))\n","\n","        # BLEU nltk\n","        bleu1_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(1, 0, 0, 0)\n","        )\n","        bleu2_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(0.5, 0.5, 0, 0)\n","        )\n","        bleu3_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(0.33, 0.33, 0.33, 0)\n","        )\n","        bleu4_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(0.25, 0.25, 0.25, 0.25)\n","        )\n","\n","        # BLEU linear BP\n","        bleu1_linear = self.linear_bleu.corpus_bleu(actual, predicted,[1,0,0,0])\n","        bleu2_linear = self.linear_bleu.corpus_bleu(actual, predicted,[0.5,0.5,0,0])\n","        bleu3_linear = self.linear_bleu.corpus_bleu(actual, predicted,[0.33,0.33,0.33,0])\n","        bleu4_linear = self.linear_bleu.corpus_bleu(actual, predicted,[0.25,0.25,0.25,0.25])\n","\n","        results = {\n","            \"BLEU-1 (nltk)\": bleu1_std,\n","            \"BLEU-2 (nltk)\": bleu2_std,\n","            \"BLEU-3 (nltk)\": bleu3_std,\n","            \"BLEU-4 (nltk)\": bleu4_std,\n","            \"BLEU-1 (linear BP)\": bleu1_linear,\n","            \"BLEU-2 (linear BP)\": bleu2_linear,\n","            \"BLEU-3 (linear BP)\": bleu3_linear,\n","            \"BLEU-4 (linear BP)\": bleu4_linear,\n","            \"METEOR\": sum(meteor_scores) / len(meteor_scores),\n","            \"Avg Pred Len\": sum(pred_lens) / len(pred_lens),\n","            \"Avg Ref Len\": sum(ref_lens) / len(ref_lens),\n","            \"Len Ratio\": (sum(pred_lens) / sum(ref_lens))\n","        }\n","\n","        print(\"\\nRESEARCH METRICS\")\n","        print(\"=\" * 50)\n","        for k, v in results.items():\n","            print(f\"{k}: {v:.4f}\")\n","        print(\"=\" * 50)\n","\n","        return results\n","\n"],"metadata":{"id":"dJ659fBI-LfX","executionInfo":{"status":"ok","timestamp":1766216279061,"user_tz":-420,"elapsed":2,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# MAIN pipeline\n","# ============================================================================\n","def main():\n","    Config.print_config()\n","\n","    # 1. features\n","    if not os.path.exists(Config.FEATURES_PATH):\n","        extractor = FeatureExtractorResNet50(device=Config.DEVICE)\n","        features = extractor.extract_features(Config.IMAGE_DIR, Config.FEATURES_PATH)\n","    else:\n","        with open(Config.FEATURES_PATH, 'rb') as f:\n","            features = pickle.load(f)\n","        print(f\"Loaded features for {len(features)} images\")\n","\n","    # 2. captions\n","    cp = CaptionProcessor(Config.CAPTION_PATH)\n","    cp.load_captions()\n","    cp.clean_captions()\n","    cp.build_vocabulary(min_freq=1)\n","    # ensure startseq/endseq exist in vocab\n","    if 'startseq' not in cp.word2idx:\n","        cp.word2idx['startseq'] = len(cp.word2idx)\n","        cp.idx2word[cp.word2idx['startseq']] = 'startseq'\n","    if 'endseq' not in cp.word2idx:\n","        cp.word2idx['endseq'] = len(cp.word2idx)\n","        cp.idx2word[cp.word2idx['endseq']] = 'endseq'\n","    cp.vocab_size = len(cp.word2idx)\n","\n","    # 3. split\n","    image_ids = list(cp.mapping.keys())\n","    random.shuffle(image_ids)\n","    split_idx = int(len(image_ids) * Config.TRAIN_SPLIT)\n","    train_ids = image_ids[:split_idx]\n","    test_ids = image_ids[split_idx:]\n","    print(f\"Train images: {len(train_ids)}, Test images: {len(test_ids)}\")\n","\n","    # 4. Datasets & loaders\n","    train_dataset = FlickrDataset(train_ids, cp.mapping, features, cp)\n","    val_dataset = FlickrDataset(test_ids, cp.mapping, features, cp)\n","    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True,\n","                              num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY)\n","    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n","                            num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY)\n","    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n","\n","    # 5. model\n","    model = ImageCaptioningModel(vocab_size=cp.vocab_size,\n","                                 embed_size=Config.EMBED_SIZE,\n","                                 num_heads=Config.NUM_HEADS,\n","                                 num_layers=Config.NUM_LAYERS,\n","                                 ff_dim=Config.FF_DIM,\n","                                 feature_dim=Config.FEATURE_SHAPE[1],\n","                                 dropout=Config.DROPOUT,\n","                                 max_len=cp.max_length)\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Total params: {total_params:,}, Trainable: {trainable_params:,}\")\n","\n","    # 6. trainer\n","    trainer = Trainer(model, train_loader, val_loader, cp, Config)\n","    trainer.train()\n","\n","    # 7. load best and evaluate\n","    checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with val loss {checkpoint['val_loss']:.4f}\")\n","\n","    evaluator = ModelEvaluator(model, cp, features, test_ids, Config)\n","    evaluator.evaluate(use_beam=True, beam_size=3)\n","\n","    research_eval = ResearchEvaluator(\n","        model=model,\n","        caption_processor=cp,\n","        features=features,\n","        image_ids=test_ids,\n","        config=Config\n","    )\n","\n","    research_results = research_eval.evaluate(\n","        use_beam=True,\n","        beam_size=3\n","    )\n","    print(f\"Research evaluation results: {research_results}\")\n","    # 8. visualize few examples\n","    generator = CaptionGenerator(model, cp, features, Config)\n","    for img_id in random.sample(test_ids, min(3, len(test_ids))):\n","        generator.visualize_caption(img_id, Config.IMAGE_DIR, beam_size=3)\n","\n"],"metadata":{"id":"TQORCPf3-Onf","executionInfo":{"status":"ok","timestamp":1766216279078,"user_tz":-420,"elapsed":6,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# convenience loader & inference\n","def load_and_inference(image_id=None):\n","    print(\"Loading model & features...\")\n","    checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n","    with open(Config.FEATURES_PATH, 'rb') as f:\n","        features = pickle.load(f)\n","    cp = CaptionProcessor(Config.CAPTION_PATH)\n","    cp.load_captions()\n","    cp.clean_captions()\n","    cp.word2idx = checkpoint['word2idx']\n","    cp.idx2word = checkpoint['idx2word']\n","    cp.vocab_size = checkpoint['vocab_size']\n","    cp.max_length = checkpoint['max_length']\n","\n","    model = ImageCaptioningModel(vocab_size=cp.vocab_size,\n","                                 embed_size=Config.EMBED_SIZE,\n","                                 num_heads=Config.NUM_HEADS,\n","                                 num_layers=Config.NUM_LAYERS,\n","                                 ff_dim=Config.FF_DIM,\n","                                 feature_dim=Config.FEATURE_SHAPE[1],\n","                                 dropout=Config.DROPOUT,\n","                                 max_len=cp.max_length)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    generator = CaptionGenerator(model, cp, features, Config)\n","    if image_id:\n","        generator.visualize_caption(image_id, Config.IMAGE_DIR, beam_size=3)\n","    else:\n","        rid = random.choice(list(cp.mapping.keys()))\n","        generator.visualize_caption(rid, Config.IMAGE_DIR, beam_size=3)\n","    return model, cp, features, generator\n","\n"],"metadata":{"id":"WIVfg7Bp-RP2","executionInfo":{"status":"ok","timestamp":1766216279079,"user_tz":-420,"elapsed":5,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDqXsqrB-x_4","executionInfo":{"status":"ok","timestamp":1766216279601,"user_tz":-420,"elapsed":524,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"9a2a5ee4-7d84-4b69-f9fb-c9841eef1718"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from google.colab import files\n","files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"kPHv_tg2DGzN","executionInfo":{"status":"error","timestamp":1766221687629,"user_tz":-420,"elapsed":5408028,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"104f4cad-5cc4-438d-d9ff-67aa4fae9aed"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-b77f4320-1c0b-425b-afe8-bec33e6b63f5\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b77f4320-1c0b-425b-afe8-bec33e6b63f5\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1613494533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    174\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Section 1: Instructions\n","    print(\"\\n========================================\")\n","    print(\"           Image Captioning CLI         \")\n","    print(\"========================================\")\n","    print(\"Available modes:\")\n","    print(\"  - 'train': Train the image captioning model.\")\n","    print(\"  - 'inference': Generate captions for an image.\")\n","    print(\"  - 'extract_features': Extract ResNet50 features from images.\")\n","    print(\"  - 'research_eval': Perform a comprehensive evaluation of the model.\")\n","    print(\"\\n--- Interactive Mode ---\")\n","\n","    # Section 2: User Input and Execution\n","    # Get mode from user with a default\n","    mode = input(\"Enter mode (train, inference, extract_features, research_eval) [default: train]: \")\n","    if not mode:\n","        mode = 'train'\n","\n","    image_id = None\n","    beam_size = 3 # Default beam size\n","\n","    if mode == 'inference':\n","        image_id_input = input(\"Enter image ID for inference (leave blank for random): \")\n","        if image_id_input:\n","            image_id = image_id_input\n","        beam_size_input = input(\"Enter beam size for inference [default: 3]: \")\n","        if beam_size_input and beam_size_input.isdigit():\n","            beam_size = int(beam_size_input)\n","\n","    elif mode == 'research_eval':\n","        beam_size_input = input(\"Enter beam size for research evaluation [default: 3]: \")\n","        if beam_size_input and beam_size_input.isdigit():\n","            beam_size = int(beam_size_input)\n","\n","    print(f\"\\nRunning in mode: {mode}\")\n","    if image_id: print(f\"Image ID: {image_id}\")\n","    if mode == 'inference' or mode == 'research_eval': print(f\"Beam Size: {beam_size}\")\n","    print(\"----------------------------------------\")\n","\n","    if mode == 'extract_features':\n","        extractor = FeatureExtractorResNet50(device=Config.DEVICE)\n","        extractor.extract_features(Config.IMAGE_DIR, Config.FEATURES_PATH)\n","    elif mode == 'train':\n","        main()\n","    elif mode == 'research_eval':\n","        if not os.path.exists(Config.MODEL_SAVE_PATH):\n","            print(\"No trained model found. Train first.\")\n","        else:\n","            checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n","\n","            with open(Config.FEATURES_PATH, 'rb') as f:\n","                features = pickle.load(f)\n","\n","            cp = CaptionProcessor(Config.CAPTION_PATH)\n","            cp.load_captions()\n","            cp.clean_captions()\n","            cp.word2idx = checkpoint['word2idx']\n","            cp.idx2word = checkpoint['idx2word']\n","            cp.vocab_size = checkpoint['vocab_size']\n","            cp.max_length = checkpoint['max_length']\n","\n","            model = ImageCaptioningModel(vocab_size=cp.vocab_size,\n","                                        embed_size=Config.EMBED_SIZE,\n","                                        num_heads=Config.NUM_HEADS,\n","                                        num_layers=Config.NUM_LAYERS,\n","                                        ff_dim=Config.FF_DIM,\n","                                        feature_dim=Config.FEATURE_SHAPE[1],\n","                                        dropout=Config.DROPOUT,\n","                                        max_len=cp.max_length)\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","\n","            # For research_eval, we typically evaluate on all available images or a specific set.\n","            # Here, we'll use all image IDs present in the captions.\n","            image_ids = list(cp.mapping.keys())\n","\n","            research_eval = ResearchEvaluator(\n","                model=model,\n","                caption_processor=cp,\n","                features=features,\n","                image_ids=image_ids,\n","                config=Config\n","            )\n","\n","            research_eval.evaluate(use_beam=True, beam_size=beam_size)\n","    elif mode == 'inference':\n","        if not os.path.exists(Config.MODEL_SAVE_PATH):\n","            print(\"No trained model found. Train first.\")\n","        else:\n","            # Pass the beam_size to load_and_inference if it supports it, or modify it to use the new beam_size variable\n","            # For now, it defaults to 3 inside load_and_inference, so we need to adjust if we want user control.\n","            # Let's adjust load_and_inference to accept beam_size.\n","            print(\"Calling load_and_inference with provided parameters...\")\n","            model_inf, cp_inf, features_inf, generator_inf = load_and_inference(image_id=image_id)\n","            if image_id:\n","                generator_inf.visualize_caption(image_id, Config.IMAGE_DIR, beam_size=beam_size)\n","            else:\n","                rid = random.choice(list(cp_inf.mapping.keys()))\n","                generator_inf.visualize_caption(rid, Config.IMAGE_DIR, beam_size=beam_size)\n","\n","    print(\"\\nDone.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlSLHb7E-S9f","outputId":"af622917-a674-46bf-ebe9-de2b0385dca3"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","========================================\n","           Image Captioning CLI         \n","========================================\n","Available modes:\n","  - 'train': Train the image captioning model.\n","  - 'inference': Generate captions for an image.\n","  - 'extract_features': Extract ResNet50 features from images.\n","  - 'research_eval': Perform a comprehensive evaluation of the model.\n","\n","--- Interactive Mode ---\n","\n","Running in mode: train\n","----------------------------------------\n","======================================================================\n","CONFIGURATION\n","======================================================================\n","Device: cpu\n","Batch Size: 32\n","Epochs: 30\n","Learning Rate: 0.0001\n","Embed Size: 512\n","Num Heads: 8\n","Num Layers: 4\n","FF Dimension: 2048\n","Dropout: 0.5\n","Feature shape: (49, 2048)\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 97.8M/97.8M [00:00<00:00, 113MB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Extracting features (ResNet50) from ./clean_data_flickr8k/Images ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Extracting features: 100%|██████████| 8091/8091 [29:33<00:00,  4.56it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Saved features -> ./clean_data_flickr8k/features_resnet50.pkl (total 8091)\n","\n","Loading captions...\n","Loaded captions for 8091 images\n","Cleaning captions...\n","Building vocabulary...\n","Vocab size: 8369, Max caption length: 34\n","Train images: 7281, Test images: 810\n","Train batches: 1138, Val batches: 127\n","Total params: 26,443,441, Trainable: 26,443,441\n","\n","======================================================================\n","STARTING TRAINING\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1/30 [Train]:   0%|          | 0/1138 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","Epoch 1/30 [Train]: 100%|██████████| 1138/1138 [1:05:34<00:00,  3.46s/it, loss=4.2321]\n","Epoch 1/30 [Val]: 100%|██████████| 127/127 [02:00<00:00,  1.05it/s, loss=3.7172]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Epoch 1/30\n","Train Loss: 4.7122 | Val Loss: 4.0067\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 4.0067\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2/30 [Train]: 100%|██████████| 1138/1138 [1:05:33<00:00,  3.46s/it, loss=3.7261]\n","Epoch 2/30 [Val]: 100%|██████████| 127/127 [01:59<00:00,  1.06it/s, loss=3.3853]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Epoch 2/30\n","Train Loss: 3.8112 | Val Loss: 3.6855\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.6855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3/30 [Train]: 100%|██████████| 1138/1138 [1:05:57<00:00,  3.48s/it, loss=3.6056]\n","Epoch 3/30 [Val]: 100%|██████████| 127/127 [02:00<00:00,  1.05it/s, loss=3.2470]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Epoch 3/30\n","Train Loss: 3.4917 | Val Loss: 3.5285\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.5285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4/30 [Train]: 100%|██████████| 1138/1138 [1:05:58<00:00,  3.48s/it, loss=3.0512]\n","Epoch 4/30 [Val]: 100%|██████████| 127/127 [02:00<00:00,  1.05it/s, loss=3.1454]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Epoch 4/30\n","Train Loss: 3.2810 | Val Loss: 3.4431\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.4431\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/30 [Train]: 100%|██████████| 1138/1138 [1:05:55<00:00,  3.48s/it, loss=3.1900]\n","Epoch 5/30 [Val]: 100%|██████████| 127/127 [02:01<00:00,  1.05it/s, loss=3.0639]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/30\n","Train Loss: 3.1168 | Val Loss: 3.3958\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.3958\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/30 [Train]: 100%|██████████| 1138/1138 [1:05:39<00:00,  3.46s/it, loss=3.1478]\n","Epoch 6/30 [Val]: 100%|██████████| 127/127 [02:01<00:00,  1.05it/s, loss=3.1194]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/30\n","Train Loss: 2.9777 | Val Loss: 3.3697\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.3697\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/30 [Train]: 100%|██████████| 1138/1138 [1:05:38<00:00,  3.46s/it, loss=2.6142]\n","Epoch 7/30 [Val]: 100%|██████████| 127/127 [02:00<00:00,  1.05it/s, loss=3.1146]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/30\n","Train Loss: 2.8598 | Val Loss: 3.3674\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.3674\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/30 [Train]: 100%|██████████| 1138/1138 [1:05:21<00:00,  3.45s/it, loss=2.7652]\n","Epoch 8/30 [Val]: 100%|██████████| 127/127 [02:00<00:00,  1.06it/s, loss=3.0668]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/30\n","Train Loss: 2.7504 | Val Loss: 3.3443\n","LR: 0.000100\n","Model saved to ./clean_data_flickr8k/best_model_resnet50_transformer.pth\n","✓ New best model! Val Loss: 3.3443\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/30 [Train]:  85%|████████▍ | 967/1138 [55:15<09:29,  3.33s/it, loss=2.3964]"]}]}]}