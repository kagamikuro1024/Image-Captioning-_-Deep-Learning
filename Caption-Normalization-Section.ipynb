{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17X_n9GBq6Kfm1btanB-W880Ozbd9_5Uf","timestamp":1763132329738},{"file_id":"1uGBhbamEbhv9ozizaHrBkSQNkOwch9V1","timestamp":1760930546150}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Tải về tập dữ liệu flickr8k đã clean"],"metadata":{"id":"Yy8yR2r1HkEd"}},{"cell_type":"code","source":["!git clone https://github.com/DoanNgocToan/clean_data_flickr8k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-vbUum6Orpl","executionInfo":{"status":"ok","timestamp":1763132842724,"user_tz":-420,"elapsed":36998,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"47b43a4f-0fa1-4f72-aab6-906b1e1ab466"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'clean_data_flickr8k'...\n","remote: Enumerating objects: 8123, done.\u001b[K\n","remote: Total 8123 (delta 0), reused 0 (delta 0), pack-reused 8123 (from 2)\u001b[K\n","Receiving objects: 100% (8123/8123), 1.03 GiB | 37.63 MiB/s, done.\n","Resolving deltas: 100% (15/15), done.\n","Updating files: 100% (8095/8095), done.\n"]}]},{"cell_type":"code","source":["!ls /content/clean_data_flickr8k/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tm6nbGQfdiWM","executionInfo":{"status":"ok","timestamp":1763132950225,"user_tz":-420,"elapsed":120,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"abc66439-b739-4195-e1f8-5fc23746994e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["captions.txt  Images  README.md  vocabulary.txt\n"]}]},{"cell_type":"markdown","source":["##Đọc file caption"],"metadata":{"id":"_ui3K7Ns5ULY"}},{"cell_type":"code","source":["import os\n","from collections import defaultdict\n","from IPython.display import display, Image\n","\n","dataset_path = '/content/clean_data_flickr8k'\n","caption_path = os.path.join(dataset_path, 'captions.txt')\n","image_dirs = os.path.join(dataset_path, 'Images')\n","# Create a directory to save image paths and captions\n","image_caption = defaultdict(list)\n","image_paths = set()\n","# Các thông số cần thiết\n","caption_counts = 0 # Số lượng caption\n","# Read the text file\n","\n","if os.path.exists(caption_path):\n","  with open(caption_path, 'r') as f:\n","    next(f) # Bỏ qua tiêu đề 'image,caption'\n","    for line in f:\n","      # Split only at the first comma to correctly separate image and caption\n","      dir_cap = line.strip().split(',', 1)\n","      image_path = os.path.join(image_dirs, dir_cap[0])\n","      image_paths.add(image_path)\n","      if os.path.exists(image_path):\n","        image_caption[image_path].append(dir_cap[1])"],"metadata":{"id":"OXMj5ZgecJ71","executionInfo":{"status":"ok","timestamp":1763132980416,"user_tz":-420,"elapsed":235,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##Tiền xử lý dữ liệu\n","\n"],"metadata":{"id":"d8Ku2rkDsNa0"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"new_cell_1","executionInfo":{"status":"ok","timestamp":1763132986006,"user_tz":-420,"elapsed":485,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"45b2f5df-7b7c-469b-b706-233f549f9e42"},"source":["import re\n","from collections import defaultdict\n","\n","# Khởi tạo lại các biến cho việc làm sạch\n","cleaned_vocab_set = set()\n","cleaned_max_lengths = 0\n","cleaned_image_caption = defaultdict(list)\n","\n","# Thực hiện làm sạch dữ liệu từ image_caption và image_paths đã có\n","for path in image_paths:\n","    if path not in image_caption:\n","        continue\n","    for caption in image_caption[path]:\n","        cleaned_words_for_caption = []\n","        raw_words = caption.split()\n","        for word in raw_words:\n","          # Loại bỏ các dấu câu thường gặp trong captions\n","            cleaned_word = word.strip(\"'.,!#():;?&- \").strip('\"').lower()\n","            if cleaned_word:\n","                cleaned_words_for_caption.append(cleaned_word)\n","\n","        # Gộp các từ đã làm sạch lại thành một caption và thêm vào cleaned_image_caption\n","        cleaned_caption_string = ' '.join(cleaned_words_for_caption)\n","        if cleaned_caption_string: # Only add non-empty cleaned captions\n","            cleaned_image_caption[path].append(cleaned_caption_string)\n","\n","        # Update max_lengths based on the length of cleaned words\n","        if cleaned_max_lengths < len(cleaned_words_for_caption):\n","           cleaned_max_lengths = len(cleaned_words_for_caption)\n","\n","        # Update cleaned_vocab_set with cleaned words\n","        cleaned_vocab_set.update(cleaned_words_for_caption)\n","\n","# In thống kê sau khi làm sạch nhẹ\n","print(\"\\n--- Thống kê sau khi làm sạch nhẹ (loại bỏ \\\".,'!#():;?) ---\")\n","print(f\"Kích thước từ vựng sau làm sạch: {len(cleaned_vocab_set)}\")\n","print(f\"Độ dài caption dài nhất sau làm sạch: {cleaned_max_lengths}\")\n","\n","# --- Tìm và in các từ chứa ký tự không phải chữ cái hoặc số ---\n","print(\"\\nCác từ trong từ vựng SAU LÀM SẠCH chứa ký tự không phải chữ cái hoặc số:\")\n","non_alphanum_words_cleaned = []\n","for word in sorted(list(cleaned_vocab_set)): # Sort for consistent output\n","    # Check if the word contains any character that is NOT a letter or a digit\n","    if re.search(r'[^a-z0-9-]', word): # Dấu '-' cuối là để loại các từ ghép có dấu gạch nối\n","        non_alphanum_words_cleaned.append(word)\n","\n","if non_alphanum_words_cleaned:\n","    for word in non_alphanum_words_cleaned:\n","        print(word)\n","else:\n","    print(\"Không có từ nào trong từ vựng sau làm sạch chứa ký tự không phải chữ cái hoặc số.\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Thống kê sau khi làm sạch nhẹ (loại bỏ \".,'!#():;?) ---\n","Kích thước từ vựng sau làm sạch: 8455\n","Độ dài caption dài nhất sau làm sạch: 36\n","\n","Các từ trong từ vựng SAU LÀM SẠCH chứa ký tự không phải chữ cái hoặc số:\n","at&t\n","avrovulcan.com\n","d.c\n","n't\n","o'clock\n","r.v\n","s.c.u.b.a\n","statefarm.com\n"]}]},{"cell_type":"markdown","metadata":{"id":"4757070a"},"source":["## Padding và Đánh Chỉ Số Cho Nhãn"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90f30033","executionInfo":{"status":"ok","timestamp":1763133000847,"user_tz":-420,"elapsed":7924,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"364fe03f-ed68-4be6-dabe-00cb3f592b25"},"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# ============================\n","# 1. Thêm các token đặc biệt vào từ vựng\n","# ============================\n","# Tạo một bản sao của tập từ vựng đã làm sạch\n","vocabulary_for_tokenizer = set(cleaned_vocab_set)\n","\n","# Thêm các token đặc biệt\n","vocabulary_for_tokenizer.add('<start>')\n","vocabulary_for_tokenizer.add('<end>')\n","vocabulary_for_tokenizer.add('<pad>')\n","\n","# Chuyển set thành list để truyền vào Tokenizer\n","vocabulary_list = sorted(list(vocabulary_for_tokenizer))\n","\n","# ============================\n","# 2. Khởi tạo Tokenizer và xây dựng chỉ mục từ\n","# ============================\n","# num_words: None (giữ tất cả các từ), oov_token: đại diện cho các từ không có trong từ vựng\n","tokenizer = Tokenizer(num_words=None, oov_token='<unk>')\n","\n","# Fit trên tập các từ vựng đã bao gồm các token đặc biệt\n","tokenizer.fit_on_texts(vocabulary_list)\n","\n","# Đảm bảo các token đặc biệt có chỉ mục cố định và là các chỉ mục thấp nhất\n","# (thường tokenizer tự làm điều này nếu chúng được thêm vào và fit_on_texts)\n","# Tuy nhiên, nếu muốn đảm bảo, có thể chỉnh sửa word_index sau khi fit.\n","# Ví dụ: nếu '<pad>' thường được gán chỉ mục 0 bởi pad_sequences, ta có thể bỏ qua.\n","# Keras pad_sequences mặc định padding_value=0, nên ta cần đảm bảo <pad> = 0 hoặc điều chỉnh padding_value.\n","# Ở đây, ta sẽ giả định tokenizer gán chỉ mục từ 1 trở đi, và pad_sequences sẽ dùng 0 cho padding.\n","\n","# Gán lại chỉ mục cho các token đặc biệt nếu cần để đảm bảo tính nhất quán\n","# Đây là một bước tùy chọn, tùy thuộc vào yêu cầu cụ thể về chỉ mục của các token đặc biệt.\n","# Trong nhiều trường hợp, việc để tokenizer tự động gán là đủ.\n","# Để đảm bảo <pad> là 0, chúng ta cần xây dựng word_index thủ công hoặc điều chỉnh sau.\n","# Cách đơn giản nhất là để pad_sequences thêm 0 và sử dụng các chỉ mục khác cho các từ.\n","# Tokenizer tự động gán 1 cho từ phổ biến nhất, 2 cho từ phổ biến thứ 2, v.v.\n","# OOV_token sẽ nhận chỉ mục cuối cùng (num_words+1 nếu num_words được set)\n","\n","# Để đảm bảo <pad> là 0, chúng ta sẽ xây dựng lại `word_index` một cách thủ công hoặc điều chỉnh nó.\n","word_index = tokenizer.word_index\n","# Tạo word_index mới với <pad> = 0, <start> = 1, <end> = 2, <unk> = 3 (hoặc các giá trị khác)\n","# Sau đó gán các từ còn lại.\n","\n","# Reset tokenizer để xây dựng thủ công\n","tokenizer = Tokenizer(num_words=None, oov_token='<unk>')\n","\n","# Tạo từ điển chỉ mục thủ công để đảm bảo vị trí các token đặc biệt\n","# Note: Keras Tokenizer mặc định các chỉ mục từ 1 trở đi. 0 được giữ cho padding.\n","# Nếu bạn muốn <pad> có chỉ mục 0, thì các từ khác sẽ bắt đầu từ 1.\n","# Chúng ta sẽ để Keras làm phần lớn, chỉ đảm bảo các token đặc biệt được xử lý.\n","# Thêm các token đặc biệt vào đầu danh sách từ vựng để chúng có chỉ mục thấp.\n","# Nhưng tốt nhất là để Tokenizer tự học và sau đó điều chỉnh (hoặc sử dụng một mapping khác).\n","\n","# Để đơn giản, ta sẽ để tokenizer tạo word_index và sau đó điều chỉnh index_word và word_index\n","# nếu muốn chỉ mục 0 là <pad>. Hoặc đơn giản hơn, thêm nó vào tập từ vựng và để tokenizer gán chỉ mục.\n","# Tokenizer sẽ gán chỉ mục từ 1 trở đi cho các từ, giữ 0 cho padding.\n","# OOV token sẽ có chỉ mục dựa trên số lượng từ. Để đơn giản, ta sẽ thêm các token này vào trước khi fit.\n","\n","# Cách phổ biến là tạo thủ công một số chỉ mục đầu tiên cho các token đặc biệt\n","# và sau đó thêm các từ còn lại.\n","\n","# Danh sách các từ có thể xuất hiện trong captions, bao gồm các token đặc biệt.\n","all_words = ['<pad>', '<start>', '<end>', '<unk>'] + sorted(list(cleaned_vocab_set))\n","\n","# Khởi tạo lại tokenizer\n","tokenizer = Tokenizer(filters='', lower=True, split=' ', oov_token='<unk>')\n","tokenizer.fit_on_texts(all_words)\n","\n","# Điều chỉnh word_index và index_word để <pad> là 0, <start> là 1, <end> là 2, <unk> là 3\n","# Keras Tokenizer tự động bỏ qua 0 cho padding, nên chỉ mục sẽ bắt đầu từ 1.\n","# Để đảm bảo, chúng ta sẽ hoán đổi nếu cần hoặc gán thủ công.\n","# Mặc định, tokenizer sẽ gán chỉ mục 1 cho từ xuất hiện nhiều nhất.\n","# Để đảm bảo các token đặc biệt có chỉ mục thấp, ta có thể xây dựng thủ công.\n","\n","# Xây dựng thủ công word_index và index_word\n","word_index = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n","index_word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n","current_index = 4\n","\n","for word in sorted(list(cleaned_vocab_set)):\n","    if word not in word_index:\n","        word_index[word] = current_index\n","        index_word[current_index] = word\n","        current_index += 1\n","\n","# Cập nhật tokenizer với word_index và index_word đã được tạo thủ công\n","tokenizer.word_index = word_index\n","tokenizer.index_word = index_word\n","\n","vocab_size = len(word_index)\n","\n","print(f\"Kích thước từ vựng (bao gồm các token đặc biệt): {vocab_size}\")\n","print(f\"Chỉ mục của <start>: {tokenizer.word_index['<start>']}\")\n","print(f\"Chỉ mục của <end>: {tokenizer.word_index['<end>']}\")\n","print(f\"Chỉ mục của <pad>: {tokenizer.word_index['<pad>']}\")\n","print(f\"Chỉ mục của <unk>: {tokenizer.word_index['<unk>']}\")\n","\n","# ============================\n","# 3. Chuyển đổi captions thành chuỗi số và padding\n","# ============================\n","# Tạo một danh sách các chuỗi caption đã được xử lý để truyền vào tokenizer\n","processed_captions = []\n","\n","# Xác định độ dài tối đa của caption sau khi thêm <start> và <end>\n","# Nó sẽ là cleaned_max_lengths + 2 (cho <start> và <end>)\n","max_caption_length = cleaned_max_lengths + 2\n","\n","for image_path, captions in cleaned_image_caption.items():\n","    for caption_text in captions:\n","        # Thêm token <start> và <end> vào caption\n","        processed_caption_with_tokens = '<start> ' + caption_text + ' <end>'\n","        processed_captions.append(processed_caption_with_tokens)\n","\n","# Chuyển đổi các caption thành chuỗi số (sequences of integers)\n","encoded_sequences = tokenizer.texts_to_sequences(processed_captions)\n","\n","# Padding các chuỗi số để chúng có cùng độ dài tối đa\n","# Post-padding để các token <end> không bị mất\n","padded_sequences = pad_sequences(encoded_sequences, maxlen=max_caption_length, padding='post', value=tokenizer.word_index['<pad>'])\n","\n","print(f\"\\nSố lượng captions đã được mã hóa: {len(padded_sequences)}\")\n","print(f\"Độ dài caption tối đa (bao gồm token đặc biệt và padding): {max_caption_length}\")\n","print(\"\\nVí dụ về một caption gốc:\", processed_captions[0])\n","print(\"Ví dụ về một caption đã được mã hóa và padding (dạng số):\")\n","print(padded_sequences[0])\n","\n","# ============================\n","# 4. Tạo từ điển ánh xạ file ảnh với các chuỗi số của caption\n","# ============================\n","image_to_padded_sequences = {}\n","caption_index = 0\n","\n","for image_path, captions in cleaned_image_caption.items():\n","    # Lấy ra các sequences đã được padding tương ứng với các captions của ảnh này\n","    num_captions_for_image = len(captions)\n","    image_to_padded_sequences[image_path] = padded_sequences[caption_index : caption_index + num_captions_for_image]\n","    caption_index += num_captions_for_image\n","\n","print(f\"\\nVí dụ về captions đã được padding cho một ảnh bất kỳ: {list(image_to_padded_sequences.keys())[0]}\")\n","print(image_to_padded_sequences[list(image_to_padded_sequences.keys())[0]])\n","\n","# Lưu trữ max_caption_length và vocab_size cho các bước sau\n","# Đây là các thông số quan trọng cho việc xây dựng mô hình\n","MAX_CAPTION_LENGTH = max_caption_length\n","VOCAB_SIZE = vocab_size\n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Kích thước từ vựng (bao gồm các token đặc biệt): 8459\n","Chỉ mục của <start>: 1\n","Chỉ mục của <end>: 2\n","Chỉ mục của <pad>: 0\n","Chỉ mục của <unk>: 3\n","\n","Số lượng captions đã được mã hóa: 40455\n","Độ dài caption tối đa (bao gồm token đặc biệt và padding): 38\n","\n","Ví dụ về một caption gốc: <start> a bird flies across the water <end>\n","Ví dụ về một caption đã được mã hóa và padding (dạng số):\n","[   1   55  730 2823   87 7518 8128    2    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0]\n","\n","Ví dụ về captions đã được padding cho một ảnh bất kỳ: /content/clean_data_flickr8k/Images/3325578605_afa7f662ec.jpg\n","[[   1   55  730 2823   87 7518 8128    2    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]\n"," [   1   55 8241  730 2823 5000   55 4023    2    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]\n"," [   1   55 8241  730 3240   65 7518 8128    2    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]\n"," [   1   55 8241 1809 2823 5000 8128    2    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]\n"," [   1 7518 4323  730 2823 4296 5000 7518 8128    2    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0e446439","executionInfo":{"status":"ok","timestamp":1763133020429,"user_tz":-420,"elapsed":106,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"6cba63f7-5a45-49d3-9b72-01b1ee0db4c3"},"source":["print(f\"Kiểu dữ liệu của padded_sequences: {type(padded_sequences)}\")\n","print(f\"Kiểu dữ liệu của các phần tử trong padded_sequences: {padded_sequences.dtype}\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Kiểu dữ liệu của padded_sequences: <class 'numpy.ndarray'>\n","Kiểu dữ liệu của các phần tử trong padded_sequences: int32\n"]}]}]}