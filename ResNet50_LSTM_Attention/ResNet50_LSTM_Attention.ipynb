{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/DoanNgocToan/clean_data_flickr8k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wq8lN6dhPtNr","executionInfo":{"status":"ok","timestamp":1766124980980,"user_tz":-420,"elapsed":33827,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"fc6b4bbb-e1c3-430d-91a6-b3858b179879"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'clean_data_flickr8k'...\n","remote: Enumerating objects: 8128, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 8128 (delta 2), reused 0 (delta 0), pack-reused 8125 (from 3)\u001b[K\n","Receiving objects: 100% (8128/8128), 1.07 GiB | 41.21 MiB/s, done.\n","Resolving deltas: 100% (15/15), done.\n","Updating files: 100% (8096/8096), done.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpEra6umOmyG"},"outputs":[],"source":["\"\"\"\n","Image Captioning with Attention (timestep attention) using PyTorch\n","Dataset: Flickr8k\n","Changes vs original:\n","- ResNet50 feature extractor -> feature maps (1, 7, 7, 2048) saved as numpy\n","- Decoder uses attention at every timestep (LSTMCell-based)\n","- Dataset returns full input_seq and target_seq (caption-level)\n","- Beam Search implemented for inference\n","Author: Refactor for user request\n","\"\"\"\n","\n","import os\n","import pickle\n","import random\n","import time\n","from collections import defaultdict\n","\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","\n","from nltk.translate.bleu_score import corpus_bleu\n"]},{"cell_type":"code","source":["\n","# ============================================================================\n","# CONFIG\n","# ============================================================================\n","class Config:\n","    DATASET_PATH = './clean_data_flickr8k'\n","    CAPTION_PATH = os.path.join(DATASET_PATH, 'captions.txt')\n","    IMAGE_DIR = os.path.join(DATASET_PATH, 'Images')\n","    FEATURES_PATH = os.path.join(DATASET_PATH, 'features_resnet50.pkl')\n","    MODEL_SAVE_PATH = os.path.join(DATASET_PATH, 'best_model_resnet50_attention.pth')\n","    TENSORBOARD_LOG_ROOT = os.path.join(DATASET_PATH, 'runs')\n","\n","    # model hyperparams\n","    EMBED_SIZE = 256\n","    HIDDEN_SIZE = 512\n","    ATTENTION_DIM = 512\n","    DROPOUT = 0.5\n","\n","    # training\n","    BATCH_SIZE = 32\n","    EPOCHS = 50\n","    LEARNING_RATE = 1e-4\n","    TRAIN_SPLIT = 0.90\n","\n","    LR_SCHEDULER_FACTOR = 0.5\n","    LR_SCHEDULER_PATIENCE = 2\n","    EARLY_STOPPING_PATIENCE = 15\n","\n","    NUM_WORKERS = 0\n","    PIN_MEMORY = False\n","\n","    # features\n","    FEATURE_SHAPE = (49, 2048)   # 7*7, 2048\n","    IMAGE_SIZE = (224, 224)\n","\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    @classmethod\n","    def print_config(cls):\n","        print(\"=\"*70)\n","        print(\"CONFIGURATION\")\n","        print(\"=\"*70)\n","        print(f\"Device: {cls.DEVICE}\")\n","        print(f\"Batch Size: {cls.BATCH_SIZE}\")\n","        print(f\"Epochs: {cls.EPOCHS}\")\n","        print(f\"Learning Rate: {cls.LEARNING_RATE}\")\n","        print(f\"Embed Size: {cls.EMBED_SIZE}\")\n","        print(f\"Hidden Size: {cls.HIDDEN_SIZE}\")\n","        print(f\"Attention Dim: {cls.ATTENTION_DIM}\")\n","        print(f\"Feature shape: {cls.FEATURE_SHAPE}\")\n","        print(\"=\"*70)"],"metadata":{"id":"LvCl1pOxP0qM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# FEATURE EXTRACTOR (ResNet50 - up to conv layer)\n","# ============================================================================\n","class FeatureExtractorResNet50:\n","    def __init__(self, device=Config.DEVICE):\n","        self.device = device\n","        self.model = self._build_model()\n","        self.transform = transforms.Compose([\n","            transforms.Resize(Config.IMAGE_SIZE),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def _build_model(self):\n","        resnet = models.resnet50(pretrained=True)\n","        # take all layers except avgpool & fc -> output: (batch, 2048, 7, 7)\n","        modules = list(resnet.children())[:-2]\n","        model = nn.Sequential(*modules)\n","        model.eval().to(self.device)\n","        for p in model.parameters():\n","            p.requires_grad = False\n","        return model\n","\n","    def extract_features(self, image_dir, save_path):\n","        print(f\"\\nExtracting features (ResNet50) from {image_dir} ...\")\n","        features = {}\n","        image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n","        with torch.no_grad():\n","            for img_name in tqdm(image_files, desc=\"Extracting features\"):\n","                img_path = os.path.join(image_dir, img_name)\n","                try:\n","                    image = Image.open(img_path).convert('RGB')\n","                    img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n","                    feat = self.model(img_tensor)  # (1, 2048, 7, 7)\n","                    feat = feat.permute(0, 2, 3, 1).cpu().numpy()  # (1,7,7,2048)\n","                    image_id = img_name.split('.')[0]\n","                    features[image_id] = feat\n","                except Exception as e:\n","                    print(f\"Error processing {img_name}: {e}\")\n","        with open(save_path, 'wb') as f:\n","            pickle.dump(features, f)\n","        print(f\"Saved features -> {save_path} (total {len(features)})\")\n","        return features\n"],"metadata":{"id":"yMY6vTzWQIr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# CAPTION PROCESSOR (same idea)\n","# ============================================================================\n","class CaptionProcessor:\n","    def __init__(self, caption_path):\n","        self.caption_path = caption_path\n","        self.mapping = {}\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.max_length = 0\n","        self.vocab_size = 0\n","\n","    def load_captions(self):\n","        print(\"\\nLoading captions...\")\n","        with open(self.caption_path, 'r', encoding='utf-8') as f:\n","            next(f)\n","            for line in f:\n","                parts = line.strip().split(',', 1)\n","                if len(parts) < 2:\n","                    continue\n","                image_name, caption = parts\n","                image_id = image_name.split('.')[0]\n","                if image_id not in self.mapping:\n","                    self.mapping[image_id] = []\n","                self.mapping[image_id].append(caption)\n","        print(f\"Loaded captions for {len(self.mapping)} images\")\n","\n","    def clean_captions(self):\n","        print(\"Cleaning captions...\")\n","        for img_id, caps in self.mapping.items():\n","            for i in range(len(caps)):\n","                cap = caps[i].lower()\n","                cap = ''.join([c for c in cap if c.isalnum() or c.isspace()])\n","                cap = ' '.join(cap.split())\n","                words = [w for w in cap.split() if len(w) > 1]\n","                caps[i] = 'startseq ' + ' '.join(words) + ' endseq'\n","\n","    def build_vocabulary(self, min_freq=1):\n","        print(\"Building vocabulary...\")\n","        freq = defaultdict(int)\n","        for caps in self.mapping.values():\n","            for cap in caps:\n","                for w in cap.split():\n","                    freq[w] += 1\n","        # optionally filter by min_freq\n","        words = [w for w, c in freq.items() if c >= min_freq]\n","        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n","        idx = 2\n","        for w in sorted(words):\n","            if w in self.word2idx:\n","                continue\n","            self.word2idx[w] = idx\n","            idx += 1\n","        self.idx2word = {i: w for w, i in self.word2idx.items()}\n","        self.vocab_size = len(self.word2idx)\n","        # max length\n","        self.max_length = max(len(c.split()) for caps in self.mapping.values() for c in caps)\n","        print(f\"Vocab size: {self.vocab_size}, Max caption length: {self.max_length}\")\n","\n","    def encode_caption(self, caption):\n","        return [self.word2idx.get(w, self.word2idx['<UNK>']) for w in caption.split()]\n","\n","    def decode_caption(self, indices):\n","        words = []\n","        for idx in indices:\n","            if idx == 0:\n","                continue\n","            w = self.idx2word.get(idx, '<UNK>')\n","            if w in ('startseq', 'endseq'):\n","                continue\n","            words.append(w)\n","        return ' '.join(words)\n"],"metadata":{"id":"FKVr2bruQZGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# DATASET (caption-level: input_seq, target_seq)\n","# ============================================================================\n","class FlickrDataset(Dataset):\n","    def __init__(self, image_ids, captions_mapping, features, caption_processor):\n","        self.image_ids = image_ids\n","        self.captions_mapping = captions_mapping\n","        self.features = features\n","        self.cp = caption_processor\n","        self.samples = []\n","        for img_id in image_ids:\n","            if img_id not in features:\n","                continue\n","            for caption in captions_mapping[img_id]:\n","                encoded = caption_processor.encode_caption(caption)\n","                # input = startseq ... token_{T-1}; target = token_1 ... endseq\n","                if len(encoded) < 2:\n","                    continue\n","                inp = encoded[:-1]\n","                tgt = encoded[1:]\n","                self.samples.append((img_id, inp, tgt))\n","        # compute max length from processor\n","        self.max_len = caption_processor.max_length - 1  # because input length is caption-1\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_id, inp, tgt = self.samples[idx]\n","        # image features: (1,7,7,2048) -> reshape to (49,2048)\n","        feat = self.features[img_id].reshape(-1, Config.FEATURE_SHAPE[1])  # (49,2048)\n","        # pad input and target to self.max_len\n","        inp_arr = np.zeros(self.max_len, dtype=np.int64)\n","        tgt_arr = np.zeros(self.max_len, dtype=np.int64)\n","        inp_arr[:len(inp)] = inp\n","        tgt_arr[:len(tgt)] = tgt\n","        length = min(len(inp), self.max_len)\n","        return (\n","            torch.FloatTensor(feat),                 # (49,2048)\n","            torch.LongTensor(inp_arr),              # (max_len,)\n","            torch.LongTensor(tgt_arr),              # (max_len,)\n","            length                                 # effective length of input (before padding)\n","        )"],"metadata":{"id":"HmMWkdcvQgoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# ATTENTION (Bahdanau) - per timestep\n","# ============================================================================\n","class Attention(nn.Module):\n","    def __init__(self, feature_dim=2048, hidden_dim=Config.HIDDEN_SIZE, attention_dim=Config.ATTENTION_DIM):\n","        super(Attention, self).__init__()\n","        self.W_feat = nn.Linear(feature_dim, attention_dim)      # projects image features\n","        self.W_hidden = nn.Linear(hidden_dim, attention_dim)     # projects hidden state\n","        self.V = nn.Linear(attention_dim, 1)                     # gives attention score\n","\n","    def forward(self, features, hidden):\n","        \"\"\"\n","        features: (batch, num_regions, feature_dim)  e.g. (B,49,2048)\n","        hidden: (batch, hidden_dim)\n","        returns: context (batch, feature_dim), alpha (batch, num_regions)\n","        \"\"\"\n","        # projects\n","        feat_proj = self.W_feat(features)          # (B,49,attn)\n","        hid_proj = self.W_hidden(hidden).unsqueeze(1)  # (B,1,attn)\n","        e = torch.tanh(feat_proj + hid_proj)       # (B,49,attn)\n","        e = self.V(e).squeeze(2)                   # (B,49)\n","        alpha = torch.softmax(e, dim=1)            # (B,49)\n","        context = torch.bmm(alpha.unsqueeze(1), features).squeeze(1)  # (B, feature_dim)\n","        return context, alpha\n"],"metadata":{"id":"kabNXj6KQpmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# MODEL: Encoder already precomputed; Decoder with LSTMCell + attention per timestep\n","# ============================================================================\n","class ImageCaptioningModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size=Config.EMBED_SIZE, hidden_size=Config.HIDDEN_SIZE,\n","                 attention_dim=Config.ATTENTION_DIM, feature_dim=2048, dropout=Config.DROPOUT):\n","        super(ImageCaptioningModel, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.feature_dim = feature_dim\n","\n","        # Embedding\n","        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n","        self.embed_dropout = nn.Dropout(dropout)\n","\n","        # Attention\n","        self.attention = Attention(feature_dim, hidden_size, attention_dim)\n","\n","        # LSTMCell: input will be [embed_t, context]\n","        self.lstm_cell = nn.LSTMCell(embed_size + feature_dim, hidden_size)\n","\n","        # initialize h,c from mean image feature\n","        self.init_h = nn.Linear(feature_dim, hidden_size)\n","        self.init_c = nn.Linear(feature_dim, hidden_size)\n","\n","        # decoder to vocab (use hidden + context)\n","        self.fc1 = nn.Linear(hidden_size + feature_dim, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc2 = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, image_features, input_seqs, lengths):\n","        \"\"\"\n","        image_features: (batch, 49, feature_dim)\n","        input_seqs: (batch, max_len)\n","        lengths: list/ tensor of actual lengths (number of tokens in input)\n","        returns: outputs: (batch, max_len, vocab_size)\n","        \"\"\"\n","        batch_size = image_features.size(0)\n","        max_len = input_seqs.size(1)\n","\n","        embeddings = self.embedding(input_seqs)  # (B, max_len, embed_size)\n","        embeddings = self.embed_dropout(embeddings)\n","\n","        # init hidden/cell\n","        mean_feats = image_features.mean(dim=1)                # (B, feature_dim)\n","        h = torch.tanh(self.init_h(mean_feats))                # (B, hidden)\n","        c = torch.tanh(self.init_c(mean_feats))                # (B, hidden)\n","\n","        outputs = torch.zeros(batch_size, max_len, self.vocab_size, device=image_features.device)\n","\n","        # iterate timesteps\n","        for t in range(max_len):\n","            # compute attention using previous hidden state h\n","            context, alpha = self.attention(image_features, h)   # (B, feature_dim), (B,49)\n","            # input to LSTMCell: concat(embed_t, context)\n","            emb_t = embeddings[:, t, :]                         # (B, embed_size)\n","            lstm_input = torch.cat([emb_t, context], dim=1)     # (B, embed+feature)\n","            h, c = self.lstm_cell(lstm_input, (h, c))           # (B, hidden)\n","            # compute output\n","            concat_h = torch.cat([h, context], dim=1)          # (B, hidden+feature)\n","            out = self.fc1(concat_h)\n","            out = self.relu(out)\n","            out = self.dropout(out)\n","            logits = self.fc2(out)                             # (B, vocab)\n","            outputs[:, t, :] = logits\n","\n","        return outputs  # (B, max_len, vocab)\n","\n"],"metadata":{"id":"TOEszKq9RnQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# TRAINER\n","# ============================================================================\n","class Trainer:\n","    def __init__(self, model, train_loader, val_loader, caption_processor, config=Config):\n","        self.model = model.to(config.DEVICE)\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.cp = caption_processor\n","        self.config = config\n","\n","        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=config.LEARNING_RATE)\n","        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, mode='min', factor=config.LR_SCHEDULER_FACTOR,\n","            patience=config.LR_SCHEDULER_PATIENCE,\n","        )\n","\n","        self.best_val_loss = float('inf')\n","        self.epochs_no_improve = 0\n","        log_dir = os.path.join(\n","            config.TENSORBOARD_LOG_ROOT,\n","            time.strftime(\"%b%d_%H-%M-%S\") + f'_{config.DEVICE.type}'\n","        )\n","        # Sử dụng log_dir mới này cho SummaryWriter\n","        self.writer = SummaryWriter(log_dir)\n","\n","    def train_epoch(self, epoch):\n","        self.model.train()\n","        total_loss = 0.0\n","        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.EPOCHS} [Train]\")\n","        for batch_idx, (img_feats, input_seqs, target_seqs, lengths) in enumerate(pbar):\n","            img_feats = img_feats.to(self.config.DEVICE)             # (B,49,2048)\n","            input_seqs = input_seqs.to(self.config.DEVICE)           # (B,max_len)\n","            target_seqs = target_seqs.to(self.config.DEVICE)         # (B,max_len)\n","            lengths = lengths.to(self.config.DEVICE)\n","\n","            outputs = self.model(img_feats, input_seqs, lengths)     # (B,max_len,vocab)\n","            # compute loss: flatten\n","            B, T, V = outputs.size()\n","            outputs_flat = outputs.view(B * T, V)\n","            targets_flat = target_seqs.view(B * T)\n","\n","            loss = self.criterion(outputs_flat, targets_flat)\n","\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n","            self.optimizer.step()\n","\n","            total_loss += loss.item()\n","            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","            global_step = epoch * len(self.train_loader) + batch_idx\n","            self.writer.add_scalar('Train/Loss_batch', loss.item(), global_step)\n","        avg_epoch_loss = total_loss / len(self.train_loader)\n","        self.writer.add_scalar('Train/Loss_epoch', avg_epoch_loss, epoch)\n","        return total_loss / len(self.train_loader)\n","\n","    def validate(self, epoch):\n","        self.model.eval()\n","        total_loss = 0.0\n","        with torch.no_grad():\n","            pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{self.config.EPOCHS} [Val]\")\n","            for img_feats, input_seqs, target_seqs, lengths in pbar:\n","                img_feats = img_feats.to(self.config.DEVICE)\n","                input_seqs = input_seqs.to(self.config.DEVICE)\n","                target_seqs = target_seqs.to(self.config.DEVICE)\n","                lengths = lengths.to(self.config.DEVICE)\n","\n","                outputs = self.model(img_feats, input_seqs, lengths)\n","                B, T, V = outputs.size()\n","                outputs_flat = outputs.view(B * T, V)\n","                targets_flat = target_seqs.view(B * T)\n","\n","                loss = self.criterion(outputs_flat, targets_flat)\n","                total_loss += loss.item()\n","                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        avg_loss = total_loss / len(self.val_loader)\n","        self.writer.add_scalar('Val/Loss_epoch', avg_loss, epoch)\n","        return avg_loss\n","\n","    def save_checkpoint(self, epoch, val_loss):\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'val_loss': val_loss,\n","            'vocab_size': self.cp.vocab_size,\n","            'max_length': self.cp.max_length,\n","            'word2idx': self.cp.word2idx,\n","            'idx2word': self.cp.idx2word\n","        }\n","        torch.save(checkpoint, self.config.MODEL_SAVE_PATH)\n","        print(f\"Model saved to {self.config.MODEL_SAVE_PATH}\")\n","\n","    def train(self):\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"STARTING TRAINING\")\n","        print(\"=\"*70)\n","        for epoch in range(self.config.EPOCHS):\n","            train_loss = self.train_epoch(epoch)\n","            val_loss = self.validate(epoch)\n","\n","            print(f\"\\nEpoch {epoch+1}/{self.config.EPOCHS}\")\n","            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","            print(f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n","\n","            self.scheduler.step(val_loss)\n","\n","            if val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.epochs_no_improve = 0\n","                self.save_checkpoint(epoch, val_loss)\n","                print(f\"✓ New best model! Val Loss: {val_loss:.4f}\")\n","            else:\n","                self.epochs_no_improve += 1\n","                print(f\"No improvement for {self.epochs_no_improve} epoch(s)\")\n","            self.writer.add_scalar('Train/LearningRate', self.optimizer.param_groups[0]['lr'], epoch)\n","\n","            if self.epochs_no_improve >= self.config.EARLY_STOPPING_PATIENCE:\n","                print(f\"Early stopping after {epoch+1} epochs\")\n","                break\n","\n","        self.writer.close()\n","        print(\"\\nTRAINING COMPLETED\\n\")\n"],"metadata":{"id":"-SBoLHiqRr3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# INFERENCE: greedy and beam search\n","# ============================================================================\n","class CaptionGenerator:\n","    def __init__(self, model, caption_processor, features, config=Config):\n","        self.model = model.to(config.DEVICE)\n","        self.model.eval()\n","        self.cp = caption_processor\n","        self.features = features\n","        self.config = config\n","\n","        # shortcuts\n","        self.start_idx = self.cp.word2idx.get('startseq', 1)\n","        self.end_idx = self.cp.word2idx.get('endseq', None)\n","        if self.end_idx is None:\n","            # try fallback\n","            self.end_idx = self.cp.word2idx.get('end', 0)\n","\n","    def _init_hidden(self, mean_feats):\n","        # mean_feats: (1, feature_dim)\n","        h = torch.tanh(self.model.init_h(mean_feats))\n","        c = torch.tanh(self.model.init_c(mean_feats))\n","        return h, c\n","\n","    def generate_caption_greedy(self, image_id, max_length=None):\n","        if max_length is None:\n","            max_length = self.cp.max_length - 1\n","        feat = self.features[image_id]  # (1,7,7,2048)\n","        feat_t = torch.FloatTensor(feat).to(self.config.DEVICE)\n","        img_feats = feat_t.reshape(1, -1, Config.FEATURE_SHAPE[1])  # (1,49,2048)\n","\n","        mean_feats = img_feats.mean(dim=1)  # (1,2048)\n","        h, c = self._init_hidden(mean_feats)\n","\n","        generated = [self.start_idx]\n","        for t in range(max_length):\n","            # attention using previous hidden\n","            context, alpha = self.model.attention(img_feats, h)\n","            # prepare embedding for last predicted token\n","            last_token = torch.LongTensor([generated[-1]]).to(self.config.DEVICE)\n","            emb = self.model.embedding(last_token).squeeze(0)  # (embed,)\n","            inp = torch.cat([emb, context.squeeze(0)], dim=0).unsqueeze(0)  # (1, embed+feat)\n","            h, c = self.model.lstm_cell(inp, (h, c))\n","            concat_h = torch.cat([h, context], dim=1)\n","            out = self.model.fc1(concat_h)\n","            out = self.model.relu(out)\n","            out = self.model.dropout(out)\n","            logits = self.model.fc2(out)  # (1, vocab)\n","            probs = torch.softmax(logits, dim=1)\n","            next_idx = probs.argmax(dim=1).item()\n","            if next_idx == self.end_idx or next_idx == 0:\n","                break\n","            generated.append(next_idx)\n","        return self.cp.decode_caption(generated)\n","\n","    def generate_caption_beam(self, image_id, beam_size=3, max_length=None):\n","        \"\"\"\n","        Beam search implementation.\n","        Each beam: (sequence_of_indices, cumulative_logprob, hidden, cell)\n","        \"\"\"\n","        if max_length is None:\n","            max_length = self.cp.max_length - 1\n","\n","        feat = self.features[image_id]\n","        feat_t = torch.FloatTensor(feat).to(self.config.DEVICE)\n","        img_feats = feat_t.reshape(1, -1, Config.FEATURE_SHAPE[1])  # (1,49,2048)\n","        mean_feats = img_feats.mean(dim=1)\n","        h0, c0 = self._init_hidden(mean_feats)\n","\n","        # initial beam\n","        beams = [([self.start_idx], 0.0, h0, c0)]\n","        completed = []\n","\n","        for _ in range(max_length):\n","            new_beams = []\n","            for seq, score, h, c in beams:\n","                last = seq[-1]\n","                if last == self.end_idx:\n","                    # already finished\n","                    completed.append((seq, score))\n","                    continue\n","                # compute attention and next logits\n","                context, alpha = self.model.attention(img_feats, h)  # (1,feat), (1,49)\n","                last_token = torch.LongTensor([last]).to(self.config.DEVICE)\n","                emb = self.model.embedding(last_token).squeeze(0)\n","                inp = torch.cat([emb, context.squeeze(0)], dim=0).unsqueeze(0)\n","                h_new, c_new = self.model.lstm_cell(inp, (h, c))\n","                concat_h = torch.cat([h_new, context], dim=1)\n","                out = self.model.fc1(concat_h)\n","                out = self.model.relu(out)\n","                out = self.model.dropout(out)\n","                logits = self.model.fc2(out)  # (1,vocab)\n","                log_probs = torch.log_softmax(logits, dim=1).squeeze(0)  # (vocab,)\n","\n","                topk_logprobs, topk_idx = torch.topk(log_probs, beam_size)\n","                for k in range(len(topk_idx)):\n","                    idx_k = int(topk_idx[k].item())\n","                    lp = float(topk_logprobs[k].item())\n","                    new_seq = seq + [idx_k]\n","                    new_score = score + lp\n","                    new_beams.append((new_seq, new_score, h_new, c_new))\n","            # select top beam_size beams\n","            new_beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n","            beams = new_beams\n","\n","            # if we have enough completed sequences, can break (optional)\n","            if len(completed) >= beam_size:\n","                break\n","\n","        # add remaining beams to completed\n","        for seq, score, _, _ in beams:\n","            completed.append((seq, score))\n","        # choose best completed by score\n","        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n","        best_seq = completed[0][0]\n","        return self.cp.decode_caption(best_seq)\n","\n","    def visualize_caption(self, image_id, image_dir, beam_size=3):\n","        # show image + print actual & predicted\n","        img_files = [f for f in os.listdir(image_dir) if f.startswith(image_id)]\n","        if not img_files:\n","            print(f\"Image {image_id} not found in {image_dir}\")\n","            return\n","        img_path = os.path.join(image_dir, img_files[0])\n","        image = Image.open(img_path)\n","        actual = self.cp.mapping.get(image_id, [])\n","        pred_greedy = self.generate_caption_greedy(image_id)\n","        pred_beam = self.generate_caption_beam(image_id, beam_size=beam_size)\n","        print(\"\\n\" + \"=\"*60)\n","        print(f\"Image ID: {image_id}\")\n","        print(\"ACTUAL CAPTIONS:\")\n","        for i, c in enumerate(actual, 1):\n","            print(f\"{i}. {c}\")\n","        print(\"\\nPREDICTED (greedy):\")\n","        print(pred_greedy)\n","        print(\"\\nPREDICTED (beam size={}):\".format(beam_size))\n","        print(pred_beam)\n","        print(\"=\"*60 + \"\\n\")\n","        try:\n","            import matplotlib.pyplot as plt\n","            plt.figure(figsize=(8,6))\n","            plt.imshow(image)\n","            plt.axis('off')\n","            plt.show()\n","        except Exception:\n","            pass\n"],"metadata":{"id":"Z_fXzOlVRwhB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# EVALUATOR\n","# ============================================================================\n","class ModelEvaluator:\n","    def __init__(self, model, caption_processor, features, image_ids, config=Config):\n","        self.generator = CaptionGenerator(model, caption_processor, features, config)\n","        self.cp = caption_processor\n","        self.image_ids = image_ids\n","\n","    def evaluate(self, use_beam=False, beam_size=3):\n","        actual = []\n","        predicted = []\n","        for image_id in tqdm(self.image_ids, desc=\"Generating captions\"):\n","            if image_id not in self.cp.mapping:\n","                continue\n","            refs = [cap.split() for cap in self.cp.mapping[image_id]]\n","            if use_beam:\n","                pred = self.generator.generate_caption_beam(image_id, beam_size).split()\n","            else:\n","                pred = self.generator.generate_caption_greedy(image_id).split()\n","            actual.append(refs)\n","            predicted.append(pred)\n","        bleu1 = corpus_bleu(actual, predicted, weights=(1,0,0,0))\n","        bleu2 = corpus_bleu(actual, predicted, weights=(0.5,0.5,0,0))\n","        bleu3 = corpus_bleu(actual, predicted, weights=(0.33,0.33,0.33,0))\n","        bleu4 = corpus_bleu(actual, predicted, weights=(0.25,0.25,0.25,0.25))\n","        print(\"\\nBLEU scores:\")\n","        print(f\"BLEU-1: {bleu1:.4f}\")\n","        print(f\"BLEU-2: {bleu2:.4f}\")\n","        print(f\"BLEU-3: {bleu3:.4f}\")\n","        print(f\"BLEU-4: {bleu4:.4f}\")\n","        return {'BLEU-1':bleu1, 'BLEU-2':bleu2, 'BLEU-3':bleu3, 'BLEU-4':bleu4}\n"],"metadata":{"id":"FFcASgcYR0Vc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# CUSTOM BLEU WITH LINEAR BP (NO NLTK)\n","# ============================================================================\n","\n","from collections import Counter\n","import math\n","\n","class LinearBPBLEU:\n","    \"\"\"\n","    Corpus-level BLEU with:\n","    - clipped n-gram precision (n=1..4)\n","    - geometric mean\n","    - linear BP = min(1, pred_len / ref_len)\n","    \"\"\"\n","\n","    def __init__(self, max_n=4):\n","        self.max_n = max_n\n","\n","    def _ngram_counts(self, tokens, n):\n","        return Counter([tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n","\n","    def _clipped_precision(self, references, hypotheses, n):\n","        \"\"\"\n","        references: list of list of tokens (multiple refs)\n","        hypotheses: list of tokens\n","        \"\"\"\n","        hyp_counts = self._ngram_counts(hypotheses, n)\n","        if not hyp_counts:\n","            return 0.0, 0\n","\n","        max_ref_counts = Counter()\n","        for ref in references:\n","            ref_counts = self._ngram_counts(ref, n)\n","            for ng in ref_counts:\n","                max_ref_counts[ng] = max(max_ref_counts[ng], ref_counts[ng])\n","\n","        clipped = {\n","            ng: min(count, max_ref_counts.get(ng, 0))\n","            for ng, count in hyp_counts.items()\n","        }\n","\n","        return sum(clipped.values()), sum(hyp_counts.values())\n","\n","    def corpus_bleu(self, list_of_references, hypotheses, weights=None):\n","        \"\"\"\n","        list_of_references: [[ref1_tokens, ref2_tokens, ...], ...]\n","        hypotheses: [hyp_tokens, ...]\n","        \"\"\"\n","        if weights is None:\n","            weights = [1 / self.max_n] * self.max_n\n","\n","        p_ns = []\n","        total_pred_len = 0\n","        total_ref_len = 0\n","\n","        for n in range(1, self.max_n + 1):\n","            clipped_total = 0\n","            total = 0\n","            for refs, hyp in zip(list_of_references, hypotheses):\n","                c, t = self._clipped_precision(refs, hyp, n)\n","                clipped_total += c\n","                total += t\n","\n","            if total == 0:\n","                p_ns.append(0.0)\n","            else:\n","                p_ns.append(clipped_total / total)\n","\n","        # geometric mean\n","        smooth_eps = 1e-9\n","        log_p_sum = 0.0\n","        for w, p in zip(weights, p_ns):\n","            log_p_sum += w * math.log(max(p, smooth_eps))\n","        geo_mean = math.exp(log_p_sum)\n","\n","        # length stats\n","        for refs, hyp in zip(list_of_references, hypotheses):\n","            total_pred_len += len(hyp)\n","            ref_lens = [len(r) for r in refs]\n","            total_ref_len += min(ref_lens, key=lambda rl: abs(rl - len(hyp)))\n","\n","        # LINEAR BP\n","        bp = min(1.0, total_pred_len / max(total_ref_len, 1))\n","\n","        return bp * geo_mean\n"],"metadata":{"id":"729mW3rbR3DF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# RESEARCH EVALUATOR: BLEU (NLTK) + BLEU (LINEAR BP) + METEOR\n","# ============================================================================\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.meteor_score import meteor_score\n","\n","class ResearchEvaluator:\n","    def __init__(self, model, caption_processor, features, image_ids, config=Config):\n","        self.generator = CaptionGenerator(model, caption_processor, features, config)\n","        self.cp = caption_processor\n","        self.image_ids = image_ids\n","        self.linear_bleu = LinearBPBLEU(max_n=4)\n","\n","    def evaluate(self, use_beam=False, beam_size=3):\n","        actual = []\n","        predicted = []\n","        meteor_scores = []\n","        pred_lens = []\n","        ref_lens = []\n","\n","        for image_id in tqdm(self.image_ids, desc=\"Research Evaluation\"):\n","            if image_id not in self.cp.mapping:\n","                continue\n","\n","            refs = [cap.split() for cap in self.cp.mapping[image_id]]\n","\n","            if use_beam:\n","                pred_tokens = self.generator.generate_caption_beam(\n","                    image_id, beam_size\n","                ).split()\n","            else:\n","                pred_tokens = self.generator.generate_caption_greedy(image_id).split()\n","\n","            actual.append(refs)\n","            predicted.append(pred_tokens)\n","\n","            # METEOR (sentence-level)\n","            meteor_scores.append(\n","                meteor_score(refs, pred_tokens)\n","            )\n","\n","            pred_lens.append(len(pred_tokens))\n","            ref_lens.append(min(len(r) for r in refs))\n","\n","        # BLEU nltk\n","        bleu1_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(1, 0, 0, 0)\n","        )\n","        bleu2_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(0.5, 0.5, 0, 0)\n","        )\n","        bleu3_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(0.33, 0.33, 0.33, 0)\n","        )\n","        bleu4_std = corpus_bleu(\n","            actual, predicted,\n","            weights=(0.25, 0.25, 0.25, 0.25)\n","        )\n","\n","        # BLEU linear BP\n","        bleu1_linear = self.linear_bleu.corpus_bleu(actual, predicted,[1,0,0,0])\n","        bleu2_linear = self.linear_bleu.corpus_bleu(actual, predicted,[0.5,0.5,0,0])\n","        bleu3_linear = self.linear_bleu.corpus_bleu(actual, predicted,[0.33,0.33,0.33,0])\n","        bleu4_linear = self.linear_bleu.corpus_bleu(actual, predicted,[0.25,0.25,0.25,0.25])\n","\n","        results = {\n","            \"BLEU-1 (nltk)\": bleu1_std,\n","            \"BLEU-2 (nltk)\": bleu2_std,\n","            \"BLEU-3 (nltk)\": bleu3_std,\n","            \"BLEU-4 (nltk)\": bleu4_std,\n","            \"BLEU-1 (linear BP)\": bleu1_linear,\n","            \"BLEU-2 (linear BP)\": bleu2_linear,\n","            \"BLEU-3 (linear BP)\": bleu3_linear,\n","            \"BLEU-4 (linear BP)\": bleu4_linear,\n","            \"METEOR\": sum(meteor_scores) / len(meteor_scores),\n","            \"Avg Pred Len\": sum(pred_lens) / len(pred_lens),\n","            \"Avg Ref Len\": sum(ref_lens) / len(ref_lens),\n","            \"Len Ratio\": (sum(pred_lens) / sum(ref_lens))\n","        }\n","\n","        print(\"\\nRESEARCH METRICS\")\n","        print(\"=\" * 50)\n","        for k, v in results.items():\n","            print(f\"{k}: {v:.4f}\")\n","        print(\"=\" * 50)\n","\n","        return results\n"],"metadata":{"id":"ywZ9W3AUR5_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# MAIN pipeline\n","# ============================================================================\n","def main():\n","    Config.print_config()\n","\n","    # 1. features\n","    if not os.path.exists(Config.FEATURES_PATH):\n","        extractor = FeatureExtractorResNet50(device=Config.DEVICE)\n","        features = extractor.extract_features(Config.IMAGE_DIR, Config.FEATURES_PATH)\n","    else:\n","        with open(Config.FEATURES_PATH, 'rb') as f:\n","            features = pickle.load(f)\n","        print(f\"Loaded features for {len(features)} images\")\n","\n","    # 2. captions\n","    cp = CaptionProcessor(Config.CAPTION_PATH)\n","    cp.load_captions()\n","    cp.clean_captions()\n","    cp.build_vocabulary(min_freq=1)\n","    # ensure startseq/endseq exist in vocab\n","    if 'startseq' not in cp.word2idx:\n","        cp.word2idx['startseq'] = len(cp.word2idx)\n","        cp.idx2word[cp.word2idx['startseq']] = 'startseq'\n","    if 'endseq' not in cp.word2idx:\n","        cp.word2idx['endseq'] = len(cp.word2idx)\n","        cp.idx2word[cp.word2idx['endseq']] = 'endseq'\n","    cp.vocab_size = len(cp.word2idx)\n","\n","    # 3. split\n","    image_ids = list(cp.mapping.keys())\n","    random.shuffle(image_ids)\n","    split_idx = int(len(image_ids) * Config.TRAIN_SPLIT)\n","    train_ids = image_ids[:split_idx]\n","    test_ids = image_ids[split_idx:]\n","    print(f\"Train images: {len(train_ids)}, Test images: {len(test_ids)}\")\n","\n","    # 4. Datasets & loaders\n","    train_dataset = FlickrDataset(train_ids, cp.mapping, features, cp)\n","    val_dataset = FlickrDataset(test_ids, cp.mapping, features, cp)\n","    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True,\n","                              num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY)\n","    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n","                            num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY)\n","    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n","\n","    # 5. model\n","    model = ImageCaptioningModel(vocab_size=cp.vocab_size,\n","                                 embed_size=Config.EMBED_SIZE,\n","                                 hidden_size=Config.HIDDEN_SIZE,\n","                                 attention_dim=Config.ATTENTION_DIM,\n","                                 feature_dim=Config.FEATURE_SHAPE[1],\n","                                 dropout=Config.DROPOUT)\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Total params: {total_params:,}, Trainable: {trainable_params:,}\")\n","\n","    # 6. trainer\n","    trainer = Trainer(model, train_loader, val_loader, cp, Config)\n","    trainer.train()\n","\n","    # 7. load best and evaluate\n","    checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with val loss {checkpoint['val_loss']:.4f}\")\n","\n","    evaluator = ModelEvaluator(model, cp, features, test_ids, Config)\n","    evaluator.evaluate(use_beam=True, beam_size=3)\n","    # Compare Linear BLEU & nltk BLEU\n","    research_eval = ResearchEvaluator(\n","        model=model,\n","        caption_processor=cp,\n","        features=features,\n","        image_ids=test_ids,\n","        config=Config\n","    )\n","\n","    research_results = research_eval.evaluate(\n","        use_beam=True,\n","        beam_size=3\n","    )\n","    print(f\"Research evaluation results: {research_results}\")\n","    # 9. visualize few examples\n","    generator = CaptionGenerator(model, cp, features, Config)\n","    for img_id in random.sample(test_ids, min(3, len(test_ids))):\n","        generator.visualize_caption(img_id, Config.IMAGE_DIR, beam_size=3)\n"],"metadata":{"id":"Cj6nqGAOR8XD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# convenience loader & inference\n","def load_and_inference(image_id=None):\n","    print(\"Loading model & features...\")\n","    checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n","    with open(Config.FEATURES_PATH, 'rb') as f:\n","        features = pickle.load(f)\n","    cp = CaptionProcessor(Config.CAPTION_PATH)\n","    cp.load_captions()\n","    cp.clean_captions()\n","    cp.word2idx = checkpoint['word2idx']\n","    cp.idx2word = checkpoint['idx2word']\n","    cp.vocab_size = checkpoint['vocab_size']\n","    cp.max_length = checkpoint['max_length']\n","\n","    model = ImageCaptioningModel(vocab_size=cp.vocab_size,\n","                                 embed_size=Config.EMBED_SIZE,\n","                                 hidden_size=Config.HIDDEN_SIZE,\n","                                 attention_dim=Config.ATTENTION_DIM,\n","                                 feature_dim=Config.FEATURE_SHAPE[1],\n","                                 dropout=Config.DROPOUT)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    generator = CaptionGenerator(model, cp, features, Config)\n","    if image_id:\n","        generator.visualize_caption(image_id, Config.IMAGE_DIR, beam_size=3)\n","    else:\n","        rid = random.choice(list(cp.mapping.keys()))\n","        generator.visualize_caption(rid, Config.IMAGE_DIR, beam_size=3)\n","    return model, cp, features, generator\n","\n"],"metadata":{"id":"DGbz3FosSBJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# NOTE BEFORE RUN\n","1. Model phụ thuộc RẤT nặng vào GPU.\n","=> ~Nên~ Hãy sử dụng v4 GPU hoặc v5e1 TPU của colab (hãy trân trọng free Pro session) hoặc đơn giản là train local khi ngủ.\n","\n","2. File feature_resnet50.pkl không nén sẽ có kích thước khoảng 3GB (!!), best_model_resnet50_attention không nén có kích thước khoảng 190MB (hãy trân trọng 4G của bạn).\n","3. Thực nghiệm khi extract feature bằng CPU colab ~ 35 phút, extract bằng RTX2050 local ~6 phút.\n","\n","4. Thực nghiệm local khi train/val mỗi epoch ~ 6-10 phút, CPU colab ước tính ~ 2,5-3h (!!).\n","\n","5. Thực nghiệm local khi tính BLEU score (nltk) ~ 1 phút, 2 BLEU ~ 2 phút; CPU colab 2 BLEU ~ 40 phút.\n","\n","6. Tổng thời gian train model thực nghiệm local (patient = 15, early stop at 24) ~3.5h.\n","\n","7. Khi tính BLEU có thể sẽ yêu cầu cài wordnet.\n","8. Thực nghiệm Linear BLEU và e-base exp BLEU (nltk) là tương đương với model này, có thể bỏ qua nếu không cần."],"metadata":{"id":"tCxRv_UkVRlR"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c500fa4","executionInfo":{"status":"ok","timestamp":1766130498610,"user_tz":-420,"elapsed":83,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"54b14d97-ce35-47de-90da-9503a219e060"},"source":["import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Section 1: Instructions\n","    print(\"\\n========================================\")\n","    print(\"           Image Captioning CLI         \")\n","    print(\"========================================\")\n","    print(\"Available modes:\")\n","    print(\"  - 'train': Train the image captioning model.\")\n","    print(\"  - 'inference': Generate captions for an image.\")\n","    print(\"  - 'extract_features': Extract ResNet50 features from images.\")\n","    print(\"  - 'research_eval': Perform a comprehensive evaluation of the model.\")\n","    print(\"\\n--- Interactive Mode ---\")\n","\n","    # Section 2: User Input and Execution\n","    # Get mode from user with a default\n","    mode = input(\"Enter mode (train, inference, extract_features, research_eval) [default: train]: \")\n","    if not mode:\n","        mode = 'train'\n","\n","    image_id = None\n","    beam_size = 3 # Default beam size\n","\n","    if mode == 'inference':\n","        image_id_input = input(\"Enter image ID for inference (leave blank for random): \")\n","        if image_id_input:\n","            image_id = image_id_input\n","        beam_size_input = input(\"Enter beam size for inference [default: 3]: \")\n","        if beam_size_input and beam_size_input.isdigit():\n","            beam_size = int(beam_size_input)\n","\n","    elif mode == 'research_eval':\n","        beam_size_input = input(\"Enter beam size for research evaluation [default: 3]: \")\n","        if beam_size_input and beam_size_input.isdigit():\n","            beam_size = int(beam_size_input)\n","\n","    print(f\"\\nRunning in mode: {mode}\")\n","    if image_id: print(f\"Image ID: {image_id}\")\n","    if mode == 'inference' or mode == 'research_eval': print(f\"Beam Size: {beam_size}\")\n","    print(\"----------------------------------------\")\n","\n","    if mode == 'extract_features':\n","        extractor = FeatureExtractorResNet50(device=Config.DEVICE)\n","        extractor.extract_features(Config.IMAGE_DIR, Config.FEATURES_PATH)\n","    elif mode == 'train':\n","        main()\n","    elif mode == 'research_eval':\n","        if not os.path.exists(Config.MODEL_SAVE_PATH):\n","            print(\"No trained model found. Train first.\")\n","        else:\n","            checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE)\n","\n","            with open(Config.FEATURES_PATH, 'rb') as f:\n","                features = pickle.load(f)\n","\n","            cp = CaptionProcessor(Config.CAPTION_PATH)\n","            cp.load_captions()\n","            cp.clean_captions()\n","            cp.word2idx = checkpoint['word2idx']\n","            cp.idx2word = checkpoint['idx2word']\n","            cp.vocab_size = checkpoint['vocab_size']\n","            cp.max_length = checkpoint['max_length']\n","\n","            model = ImageCaptioningModel(\n","                vocab_size=cp.vocab_size,\n","                embed_size=Config.EMBED_SIZE,\n","                hidden_size=Config.HIDDEN_SIZE,\n","                attention_dim=Config.ATTENTION_DIM,\n","                feature_dim=Config.FEATURE_SHAPE[1],\n","                dropout=Config.DROPOUT\n","            )\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","\n","            # For research_eval, we typically evaluate on all available images or a specific set.\n","            # Here, we'll use all image IDs present in the captions.\n","            image_ids = list(cp.mapping.keys())\n","\n","            research_eval = ResearchEvaluator(\n","                model=model,\n","                caption_processor=cp,\n","                features=features,\n","                image_ids=image_ids,\n","                config=Config\n","            )\n","\n","            research_eval.evaluate(use_beam=True, beam_size=beam_size)\n","    elif mode == 'inference':\n","        if not os.path.exists(Config.MODEL_SAVE_PATH):\n","            print(\"No trained model found. Train first.\")\n","        else:\n","            # Pass the beam_size to load_and_inference if it supports it, or modify it to use the new beam_size variable\n","            # For now, it defaults to 3 inside load_and_inference, so we need to adjust if we want user control.\n","            # Let's adjust load_and_inference to accept beam_size.\n","            print(\"Calling load_and_inference with provided parameters...\")\n","            model_inf, cp_inf, features_inf, generator_inf = load_and_inference(image_id=image_id)\n","            if image_id:\n","                generator_inf.visualize_caption(image_id, Config.IMAGE_DIR, beam_size=beam_size)\n","            else:\n","                rid = random.choice(list(cp_inf.mapping.keys()))\n","                generator_inf.visualize_caption(rid, Config.IMAGE_DIR, beam_size=beam_size)\n","\n","    print(\"\\nDone.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":781},"id":"hyE1vQ40SDFr","executionInfo":{"status":"error","timestamp":1766130706110,"user_tz":-420,"elapsed":207499,"user":{"displayName":"Toàn Đoàn Ngọc","userId":"13017170633569405622"}},"outputId":"b6e2d8ab-11a5-463e-e019-ef79cdb19a9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","========================================\n","           Image Captioning CLI         \n","========================================\n","Available modes:\n","  - 'train': Train the image captioning model.\n","  - 'inference': Generate captions for an image.\n","  - 'extract_features': Extract ResNet50 features from images.\n","  - 'research_eval': Perform a comprehensive evaluation of the model.\n","\n","--- Interactive Mode ---\n","Enter mode (train, inference, extract_features, research_eval) [default: train]: research_eval\n","Enter beam size for research evaluation [default: 3]: 3\n","\n","Running in mode: research_eval\n","Beam Size: 3\n","----------------------------------------\n","\n","Loading captions...\n","Loaded captions for 8091 images\n","Cleaning captions...\n"]},{"output_type":"stream","name":"stderr","text":["Research Evaluation:   7%|▋         | 530/8091 [02:50<40:26,  3.12it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4169631098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m             )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mresearch_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_beam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'inference'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2734468155.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, use_beam, beam_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_beam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 pred_tokens = self.generator.generate_caption_beam(\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 ).split()\n","\u001b[0;32m/tmp/ipython-input-390978941.py\u001b[0m in \u001b[0;36mgenerate_caption_beam\u001b[0;34m(self, image_id, beam_size, max_length)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mh_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mconcat_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1705\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_batched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m         ret = _VF.lstm_cell(\n\u001b[0m\u001b[1;32m   1708\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}