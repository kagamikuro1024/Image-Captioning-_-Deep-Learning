# Image Captioning with Deep Learning
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Automatic Image Captioning** using Encoder-Decoder Architecture with CNN and LSTM/Attention Mechanisms

## ğŸ“‘ Má»¥c lá»¥c
- [Tá»•ng quan](#-tá»•ng-quan)
- [Dá»¯ liá»‡u](#-dá»¯-liá»‡u)
- [Kiáº¿n trÃºc mÃ´ hÃ¬nh](#-kiáº¿n-trÃºc-mÃ´-hÃ¬nh)
- [Káº¿t quáº£](#-káº¿t-quáº£)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u trÃºc thÆ° má»¥c](#-cáº¥u-trÃºc-thÆ°-má»¥c)
- [Tham kháº£o](#-tham-kháº£o)

---

## ğŸ¯ Tá»•ng quan

Dá»± Ã¡n nÃ y triá»ƒn khai **bÃ i toÃ¡n sinh mÃ´ táº£ áº£nh tá»± Ä‘á»™ng (Image Captioning)** sá»­ dá»¥ng kiáº¿n trÃºc **Encoder-Decoder**. Há»‡ thá»‘ng cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng táº¡o ra cÃ¡c cÃ¢u mÃ´ táº£ báº±ng tiáº¿ng Anh cho hÃ¬nh áº£nh Ä‘áº§u vÃ o, káº¿t há»£p giá»¯a:

- **Computer Vision**: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng hÃ¬nh áº£nh
- **Natural Language Processing**: Sinh chuá»—i vÄƒn báº£n mÃ´ táº£

### Äiá»ƒm ná»•i báº­t

âœ… Há»— trá»£ nhiá»u kiáº¿n trÃºc **CNN backbone** (ResNet50, EfficientNet B2-B4)  
âœ… Triá»ƒn khai **LSTM vá»›i Attention Mechanism**  
âœ… Huáº¥n luyá»‡n trÃªn cáº£ **Flickr8k** (8K áº£nh) vÃ  **Flickr30k** (31K áº£nh)  
âœ… ÄÃ¡nh giÃ¡ chi tiáº¿t vá»›i **BLEU Score** vÃ  **METEOR**  
âœ… Há»— trá»£ **Beam Search** vÃ  **Greedy Decoding**  
âœ… **Transfer Learning** tá»« ImageNet pretrained models

---

## ğŸ“Š Dá»¯ liá»‡u

### Datasets

Dá»± Ã¡n sá»­ dá»¥ng hai bá»™ dá»¯ liá»‡u chuáº©n:

| Dataset | Sá»‘ áº£nh | Sá»‘ captions | Avg captions/image |
|---------|--------|-------------|-------------------|
| **Flickr8k** | 8,091 | 40,455 | 5 |
| **Flickr30k** | 31,783 | 158,915 | 5 |

### Cáº¥u trÃºc dá»¯ liá»‡u

```
dataset/
â”œâ”€â”€ Images/                # ThÆ° má»¥c chá»©a áº£nh
â”‚   â””â”€â”€ flickr30k_images/
â””â”€â”€ captions.txt          # File chá»©a captions (Flickr8k)
â””â”€â”€ results.csv           # File chá»©a captions (Flickr30k)
```

### Äá»‹nh dáº¡ng Caption File

**Flickr8k** (`captions.txt`):
```
image,caption
1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .
```

**Flickr30k** (`results.csv`):
```
image_name | comment_number | comment
1000092795.jpg | 0 | Two young guys with shaggy hair look at their hands while hanging out in the yard .
```

### Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

#### 1. Tiá»n xá»­ lÃ½ áº£nh (Image Preprocessing)

- **Resize**: Äiá»u chá»‰nh kÃ­ch thÆ°á»›c theo yÃªu cáº§u cá»§a tá»«ng backbone
  - ResNet50: `224Ã—224`
  - EfficientNet-B2: `260Ã—260`
  - EfficientNet-B3: `300Ã—300`
  - EfficientNet-B4: `380Ã—380`
  
- **Normalization**: Chuáº©n hÃ³a theo ImageNet statistics
  ```python
  mean = [0.485, 0.456, 0.406]
  std = [0.229, 0.224, 0.225]
  ```

- **Feature Extraction**: TrÃ­ch xuáº¥t feature maps tá»« CNN pretrained
  - ResNet50: `(49, 2048)` â†’ 7Ã—7 spatial grid
  - EfficientNet-B2: `(81, 1408)` â†’ 9Ã—9 spatial grid
  - EfficientNet-B3: `(100, 1536)` â†’ 10Ã—10 spatial grid
  - EfficientNet-B4: `(144, 1792)` â†’ 12Ã—12 spatial grid

#### 2. Tiá»n xá»­ lÃ½ vÄƒn báº£n (Text Preprocessing)

- **Cleaning**:
  - Chuyá»ƒn vá» chá»¯ thÆ°á»ng (lowercase)
  - Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  sá»‘
  - Loáº¡i bá» tá»« cÃ³ Ä‘á»™ dÃ i â‰¤ 1

- **Special Tokens**:
  ```
  <PAD>: Padding token (index 0)
  <UNK>: Unknown words (index 1)
  startseq: Báº¯t Ä‘áº§u cÃ¢u
  endseq: Káº¿t thÃºc cÃ¢u
  ```

- **Vocabulary Building**:
  - Flickr8k: 8,369 tá»« (min_freq=1)
  - Flickr30k: 20,157 tá»« (min_freq=2)
  - Max caption length: 34-40 tá»«

- **Padding**: ÄÆ°a táº¥t cáº£ sequences vá» cÃ¹ng Ä‘á»™ dÃ i báº±ng `<PAD>` token

---

## ğŸ—ï¸ Kiáº¿n trÃºc mÃ´ hÃ¬nh

### Tá»•ng quan Encoder-Decoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     IMAGE CAPTIONING PIPELINE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  [Input Image]                                                â”‚
â”‚       â”‚                                                       â”‚
â”‚       â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚  CNN Encoder    â”‚  (ResNet50 / EfficientNet)              â”‚
â”‚  â”‚  - Pretrained   â”‚                                         â”‚
â”‚  â”‚  - Feature Maps â”‚                                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚           â”‚ Feature Vector (NÃ—D)                             â”‚
â”‚           â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚  LSTM Decoder   â”‚                                         â”‚
â”‚  â”‚  + Attention    â”‚                                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚           â”‚                                                   â”‚
â”‚           â–¼                                                   â”‚
â”‚  [Generated Caption]                                          â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Encoder: CNN Feature Extractor

#### ResNet50
- **Pretrained**: ImageNet (1000 classes)
- **Architecture**: 50 layers vá»›i residual connections
- **Output**: `(batch, 2048, 7, 7)` â†’ Reshape to `(batch, 49, 2048)`
- **Params**: ~23M (frozen)

#### EfficientNet Family
- **Compound Scaling**: Tá»‘i Æ°u depth, width, resolution Ä‘á»“ng thá»i
- **Variants**:

| Model | Input Size | Feature Map | Feature Dim | Params |
|-------|-----------|-------------|-------------|---------|
| EfficientNet-B2 | 260Ã—260 | 9Ã—9 | 1408 | ~7.7M |
| EfficientNet-B3 | 300Ã—300 | 10Ã—10 | 1536 | ~10.7M |
| EfficientNet-B4 | 380Ã—380 | 12Ã—12 | 1792 | ~17.7M |

### 2. Decoder: LSTM with Attention

#### Bahdanau Attention Mechanism

```python
# Attention computation táº¡i má»—i timestep
Î±_t = softmax(v^T * tanh(W_encoder * encoder_out + W_decoder * h_{t-1}))
context_t = Î£(Î±_t * encoder_out)  # Weighted sum
```

**Æ¯u Ä‘iá»ƒm**:
- Táº­p trung vÃ o cÃ¡c vÃ¹ng quan trá»ng cá»§a áº£nh táº¡i má»—i bÆ°á»›c sinh tá»«
- Giáº£i quyáº¿t váº¥n Ä‘á» information bottleneck
- Cáº£i thiá»‡n kháº£ nÄƒng mÃ´ táº£ chi tiáº¿t

#### LSTM Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LSTM Decoder with Attention             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Encoder Features (NÃ—D)                             â”‚
â”‚         â”‚                                            â”‚
â”‚         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  Attention   â”‚ â—„â”€â”€â”€ h_{t-1}                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚ context_t                                 â”‚
â”‚         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  Embedding   â”‚ â—„â”€â”€â”€ word_{t-1}                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                            â”‚
â”‚         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  LSTMCell    â”‚                                   â”‚
â”‚  â”‚  h_t, c_t    â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                            â”‚
â”‚         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  Dropout     â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                            â”‚
â”‚         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  FC + Softmaxâ”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                            â”‚
â”‚         â–¼                                            â”‚
â”‚      word_t                                          â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Hyperparameters**:
```python
EMBED_SIZE = 512        # Word embedding dimension
HIDDEN_SIZE = 512       # LSTM hidden state size
ATTENTION_DIM = 512     # Attention layer dimension

EMBED_DROPOUT = 0.4     # Embedding dropout
LSTM_DROPOUT = 0.3      # LSTM dropout
DECODER_DROPOUT = 0.5   # Output dropout
```

### 3. Loss Function & Optimization

#### Loss Function
- **CrossEntropyLoss** vá»›i **Label Smoothing** (0.1)
- Bá» qua `<PAD>` tokens trong tÃ­nh toÃ¡n loss

```python
criterion = nn.CrossEntropyLoss(
    ignore_index=0,      # Ignore <PAD>
    label_smoothing=0.1  # Reduce overconfidence
)
```

#### Optimizer
- **Adam** optimizer
- Learning rate: `1e-4` (Flickr8k), `3e-4` (Flickr30k)
- Weight decay: `1e-5`

#### Learning Rate Scheduler
- **ReduceLROnPlateau**
- Factor: 0.7
- Patience: 1-2 epochs
- Giáº£m LR khi validation loss khÃ´ng cáº£i thiá»‡n

#### Early Stopping
- Patience: 5 epochs
- Dá»«ng training khi validation loss khÃ´ng giáº£m

#### Gradient Clipping
- Clip norm: 5.0
- TrÃ¡nh gradient explosion

---

## ğŸ“ˆ Káº¿t quáº£

### So sÃ¡nh cÃ¡c mÃ´ hÃ¬nh trÃªn Flickr8k

| Model | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | Epochs |
|-------|--------|--------|--------|--------|---------|---------|
| **ResNet50 + LSTM + Attention** | 0.5166 | 0.3546 | 0.2372 | **0.1491** | - | 9 |
| **EfficientNet-B2 + LSTM + Attention** | 0.5061 | 0.3404 | 0.2273 | 0.1453 | 0.2949 | 12 |
| **EfficientNet-B3 + LSTM + Attention** | **0.5243** | **0.3540** | **0.2363** | 0.1507 | **0.3065** | 14 |
| ResNet50 + Transformer | 0.4807 | 0.3307 | 0.2207 | 0.1392 | - | - |

### Káº¿t quáº£ trÃªn Flickr30k

| Model | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | Epochs |
|-------|--------|--------|--------|--------|---------|---------|
| **ResNet50 + LSTM + Attention** | 0.5034 | 0.3268 | 0.2149 | 0.1319 | 0.2662 | 22 |

### ÄÃ¡nh giÃ¡ trÃªn toÃ n bá»™ training set

| Model | Dataset | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR |
|-------|---------|--------|--------|--------|--------|---------|
| ResNet50 | Flickr8k | 0.5995 | 0.4425 | 0.3210 | 0.2227 | 0.3608 |
| ResNet50 | Flickr30k | 0.5746 | 0.4071 | 0.2870 | 0.1943 | 0.3213 |

### Nháº­n xÃ©t

#### âœ… Káº¿t quáº£ tá»‘t nháº¥t
**EfficientNet-B3 + LSTM + Attention** Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn Flickr8k:
- BLEU-1: **0.5243** (cao nháº¥t)
- METEOR: **0.3065** (cao nháº¥t)
- CÃ¢n báº±ng tá»‘t giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng tá»•ng quÃ¡t

#### ğŸ“Š PhÃ¢n tÃ­ch

1. **EfficientNet vs ResNet**:
   - EfficientNet-B3 vÆ°á»£t trá»™i vá» BLEU-1 vÃ  METEOR
   - ResNet50 cÃ³ BLEU-4 cao hÆ¡n má»™t chÃºt (0.1491 vs 0.1507)
   - EfficientNet hiá»‡u quáº£ hÆ¡n vá»›i sá»‘ params Ã­t hÆ¡n

2. **LSTM + Attention vs Transformer**:
   - LSTM + Attention vÆ°á»£t trá»™i rÃµ rá»‡t
   - Transformer Ä‘Æ¡n giáº£n chÆ°a Ä‘áº¡t hiá»‡u quáº£ (cáº§n thÃªm tricks)

3. **Flickr8k vs Flickr30k**:
   - Training trÃªn Flickr8k cho káº¿t quáº£ test tá»‘t hÆ¡n (overfitting Ã­t hÆ¡n)
   - Flickr30k cáº§n nhiá»u epochs vÃ  regularization hÆ¡n

### VÃ­ dá»¥ dá»± Ä‘oÃ¡n

#### âœ… TrÆ°á»ng há»£p tá»‘t
```
Image: 3066429707_842e50b8f7.jpg
Ground Truth: "girl in blue kicks the soccer ball"
Predicted: "girl in red shirt is playing soccer"
â†’ Nháº­n diá»‡n Ä‘Ãºng: girl, playing soccer
```

#### âš ï¸ TrÆ°á»ng há»£p cáº§n cáº£i thiá»‡n
```
Image: 476740978_45b65ebe0c.jpg
Ground Truth: "people holding pink signs that spell out impeach"
Predicted: "group of people stand on the street"
â†’ Thiáº¿u chi tiáº¿t: signs, impeach
```

---

## ğŸ”§ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng

- **OS**: Windows 10/11, Linux, macOS
- **Python**: 3.8+
- **GPU**: NVIDIA GPU vá»›i CUDA support (khuyáº¿n nghá»‹)
  - RTX 3050 Ti 4GB: Batch size 12-32
  - RTX 3060 6GB+: Batch size 32-64
- **RAM**: 16GB+ (32GB khuyáº¿n nghá»‹ cho Flickr30k)
- **Storage**: 10GB+ free space

### CÃ i Ä‘áº·t dependencies

```bash
# Clone repository
git clone https://github.com/yourusername/image-captioning.git
cd image-captioning

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
conda create -n image_caption python=3.9
conda activate image_caption

# Hoáº·c dÃ¹ng venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# CÃ i Ä‘áº·t PyTorch vá»›i CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n khÃ¡c
pip install -r requirements.txt
```

### requirements.txt
```txt
torch>=2.0.0
torchvision>=0.15.0
Pillow>=9.0.0
numpy>=1.21.0
tqdm>=4.62.0
nltk>=3.6.0
tensorboard>=2.11.0
```

### Download datasets

#### Flickr8k
```bash
# Download from Kaggle
# https://www.kaggle.com/datasets/adityajn105/flickr8k

# Hoáº·c dÃ¹ng Kaggle API
kaggle datasets download -d adityajn105/flickr8k
unzip flickr8k.zip -d content/clean_data_flickr8k/
```

#### Flickr30k
```bash
# Download from Kaggle
# https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset

kaggle datasets download -d hsankesara/flickr-image-dataset
unzip flickr-image-dataset.zip -d content/clean_data_flickr30k/
```

### Download pretrained models (Optional)

```bash
# Download best model weights
wget https://your-link/best_model_efficientnet_b3_h512.pth -P content/clean_data_flickr8k/
```

---

## ğŸš€ Sá»­ dá»¥ng

### 1. TrÃ­ch xuáº¥t features (Láº§n Ä‘áº§u tiÃªn)

```bash
# Vá»›i ResNet50
python ResNet50_LSTM_Attention/ResNet50_LSTM_Attention_Good.py --mode extract

# Vá»›i EfficientNet-B3
python EfficientNet_LSTM_Attention/EfficientNet_Full_Flavor_LSTM_Attention.py --mode extract
```

**LÆ°u Ã½**: QuÃ¡ trÃ¬nh nÃ y sáº½ táº¡o file `.pkl` chá»©a features (cÃ³ thá»ƒ lá»›n hÆ¡n 4GB):
- ResNet50 Flickr8k: ~4GB
- EfficientNet-B3 Flickr8k: ~4.7GB
- EfficientNet-B3 Flickr30k: ~18GB

### 2. Training

#### ResNet50 + LSTM + Attention (Flickr8k)

```bash
cd ResNet50_LSTM_Attention
python ResNet50_LSTM_Attention_Good.py --mode train
```

**Thá»i gian training**: ~5 phÃºt/epoch trÃªn RTX 3050 Ti (30 epochs = ~2.5 giá»)

#### EfficientNet-B3 + LSTM + Attention (Flickr8k)

```bash
cd EfficientNet_LSTM_Attention

# Chá»‰nh sá»­a Config trong file .py:
# EFFICIENTNET_VARIANT = 'b3'  # Chá»n 'b2', 'b3', or 'b4'
# BATCH_SIZE = 32              # Äiá»u chá»‰nh theo VRAM

python EfficientNet_Full_Flavor_LSTM_Attention.py --mode train
```

**Thá»i gian training**: ~4-5 phÃºt/epoch (40 epochs = ~3 giá»)

#### ResNet50 + LSTM + Attention (Flickr30k)

```bash
cd ResNet50_LSTM_Attention
python ResNet50_LSTM_Attention_Good.py --mode train

# Hoáº·c cháº¡y riÃªng cho Flickr30k
python ResNet50_LSTM_Attention_Flickr30k.py --mode train
```

**Thá»i gian training**: ~23-25 phÃºt/epoch (50 epochs = ~20 giá»)

### 3. Evaluation

```bash
# ÄÃ¡nh giÃ¡ trÃªn test set
python ResNet50_LSTM_Attention_Good.py --mode eval

# ÄÃ¡nh giÃ¡ chi tiáº¿t vá»›i METEOR
python ResNet50_LSTM_Attention_Good.py --mode research_eval

# Xem TensorBoard logs
tensorboard --logdir=content/clean_data_flickr8k/runs
```

### 4. Inference - Sinh caption cho áº£nh má»›i

```python
from PIL import Image
from EfficientNet_Full_Flavor_LSTM_Attention import *

# Load model
config = Config()
model = load_model(config)

# Load vÃ  preprocess áº£nh
image = Image.open('path/to/your/image.jpg')

# Generate caption vá»›i Greedy Decoding
caption_greedy = generate_caption_greedy(model, image, vocab)
print(f"Greedy: {caption_greedy}")

# Generate caption vá»›i Beam Search (tá»‘t hÆ¡n)
caption_beam = generate_caption_beam(model, image, vocab, beam_size=3)
print(f"Beam Search (k=3): {caption_beam}")
```

### 5. Testing script

```bash
# Test vá»›i má»™t áº£nh cá»¥ thá»ƒ
cd EfficientNet_LSTM_Attention
python Test_Image_Caption.py --image path/to/image.jpg --model b3
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
image-captioning/
â”‚
â”œâ”€â”€ EfficientNet_LSTM_Attention/          # EfficientNet models
â”‚   â”œâ”€â”€ EfficientNet_Full_Flavor_LSTM_Attention.py  # Main training script
â”‚   â”œâ”€â”€ EfficientNet_Kaggle.ipynb        # Jupyter notebook version
â”‚   â”œâ”€â”€ Test_Image_Caption.py            # Inference script
â”‚   â”œâ”€â”€ check_dataset.py                 # Dataset verification
â”‚   â”œâ”€â”€ requirements.txt                 # Dependencies
â”‚   â”œâ”€â”€ EfficientNetB2_LSTM_Attention.txt  # B2 training log
â”‚   â”œâ”€â”€ EfficientNetB3_LSTM_Attention.txt  # B3 training log
â”‚   â”œâ”€â”€ EfficientNetB4_LSTM_Attention.txt  # B4 training log
â”‚   â”œâ”€â”€ best_model_efficientnet_b3_h512.pth  # Best model weights
â”‚   â””â”€â”€ content/
â”‚       â””â”€â”€ clean_data_flickr30k/        # Flickr30k dataset
â”‚           â”œâ”€â”€ Images/
â”‚           â”œâ”€â”€ results.csv
â”‚           â”œâ”€â”€ features_efficientnet_b3.pkl
â”‚           â””â”€â”€ runs/                    # TensorBoard logs
â”‚
â”œâ”€â”€ ResNet50_LSTM_Attention/              # ResNet50 models
â”‚   â”œâ”€â”€ ResNet50_LSTM_Attention_Good.py  # Main script (Flickr8k)
â”‚   â”œâ”€â”€ ResNet50_LSTM_Attention.ipynb    # Jupyter notebook
â”‚   â”œâ”€â”€ ResNet50_LSTM_Attention.txt      # Flickr8k training log
â”‚   â”œâ”€â”€ ResNet50_30k.txt                 # Flickr30k training log
â”‚   â””â”€â”€ Dec21_*/                         # Training runs
â”‚
â”œâ”€â”€ ResNet50_Transformer_Simple/          # Transformer experiments
â”‚   â”œâ”€â”€ ResNet50_Transformer.py
â”‚   â”œâ”€â”€ ResNet50_Transformer.ipynb
â”‚   â””â”€â”€ ResNet50_Transformer.txt
â”‚
â”œâ”€â”€ BÃ¡o cÃ¡o Há»c sÃ¢u/                      # LaTeX report
â”‚   â”œâ”€â”€ tr21-60.tex                      # Main LaTeX file
â”‚   â”œâ”€â”€ svmult.cls                       # Document class
â”‚   â””â”€â”€ chapters/                        # Report chapters
â”‚
â”œâ”€â”€ Caption-Normalization-Section.ipynb   # Data preprocessing
â”œâ”€â”€ ImageCaptioning.ipynb                 # Overview notebook
â”œâ”€â”€ README.md                             # This file
â””â”€â”€ requirements.txt                      # Global dependencies
```

---

## ğŸ“š Tham kháº£o

### Papers

1. **Show, Attend and Tell** (Xu et al., 2015)
   - Attention mechanism for image captioning
   - [arXiv:1502.03044](https://arxiv.org/abs/1502.03044)

2. **Deep Residual Learning** (He et al., 2016)
   - ResNet architecture
   - [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)

3. **EfficientNet: Rethinking Model Scaling** (Tan & Le, 2019)
   - EfficientNet compound scaling
   - [arXiv:1905.11946](https://arxiv.org/abs/1905.11946)

4. **BLEU: a Method for Automatic Evaluation** (Papineni et al., 2002)
   - BLEU score metric

5. **METEOR: An Automatic Metric for MT Evaluation** (Banerjee & Lavie, 2005)
   - METEOR score metric

### Datasets

- **Flickr8k**: [Kaggle Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)
- **Flickr30k**: [Kaggle Dataset](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset)

### Code References

- PyTorch Official Tutorials
- [Show and Tell Implementation](https://github.com/yunjey/pytorch-tutorial)

---

## ğŸ“ Contributors

- **Táº¡ Quá»‘c Tuáº¥n** - Team Lead
- **Phan Trá»ng Äá»©c** - Architecture Design
- **ÄoÃ n Ngá»c ToÃ n** - Implementation
- **LÃª VÄƒn Quang Trung** - Evaluation

**TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin - ÄHQG TP.HCM**

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Äá» tÃ i bÃ i táº­p lá»›n mÃ´n **Há»c SÃ¢u (Deep Learning)**
- Cáº£m Æ¡n Kaggle vÃ  cá»™ng Ä‘á»“ng open-source
- Pretrained models tá»« PyTorch Model Zoo

---

## ğŸ“§ Contact

Náº¿u cÃ³ cÃ¢u há»i hoáº·c gÃ³p Ã½, vui lÃ²ng liÃªn há»‡:
- Email: [your.email@example.com]
- Issues: [GitHub Issues](https://github.com/yourusername/image-captioning/issues)

---

**â­ Náº¿u tháº¥y dá»± Ã¡n há»¯u Ã­ch, hÃ£y cho chÃºng tÃ´i má»™t star!**
